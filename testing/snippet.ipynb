{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f221e9-8afb-4f1a-bae1-0c38b5247c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mooring, can we just interpolate the gridded fields directly rather than interpolating to points and then re-gridding?\n",
    "# YES but it's slower (!?!)\n",
    "X = ds.XC.isel(time=0) \n",
    "Y = ds.YC.isel(time=0)\n",
    "i = ds.i\n",
    "j = ds.j\n",
    "z = ds.Z.isel(time=0)\n",
    "k = ds.k\n",
    "f_x = interpolate.interp1d(X[0,:].values, i)\n",
    "f_y = interpolate.interp1d(Y[:,0].values, j)\n",
    "f_z = interpolate.interp1d(z, k, bounds_error=False)\n",
    "\n",
    "#\n",
    "model_xav = ds.XC.isel(time=0, j=0).mean(dim='i').values\n",
    "model_yav = ds.YC.isel(time=0, i=0).mean(dim='j').values\n",
    "xmooring = model_xav # default lat/lon is the center of the domain\n",
    "ymooring = model_yav\n",
    "zmooring_TS = [-1, -10, -50, -100] # depth of T/S instruments\n",
    "zmooring_UV = [-1, -10, -50, -100] # depth of U/V instruments\n",
    "\n",
    "#\n",
    "# ds.Theta.sel(k=1)\n",
    "# ds.Theta.sel(k=1)\n",
    "ki = f_z(zmooring_TS[0])\n",
    "ii = f_x(xmooring)\n",
    "ji = f_y(ymooring)\n",
    "xo = ds.interp(i=ii, j=ji, k=ki)\n",
    "xo.Theta.plot.line(x='time')\n",
    "sdata.Theta.where(sdata.dep == sdata.dep.max(), drop=True).plot.line(x='time')\n",
    "sgridded.Theta.isel(depth=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb36a2-2a58-4e92-901b-54d76ab5def3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c3ca7-0570-4bc7-b83e-0d1d055aecb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175777d-299b-4bb3-9ec0-df0bea304c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b637c-b7c1-4432-afd6-30668d68c7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c5670d-ee93-45e8-8474-70b71ebed5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the above regridding is correct by plotting the data at one level as a time series\n",
    "# YES.\n",
    "\n",
    "# shallowest (max because negative)\n",
    "zpl = np.max(sdata['dep'].data)\n",
    "print(zpl)\n",
    "\n",
    "# select data at one depth:\n",
    "this_var_onedep = sdata[vbl].where(sdata.dep == zpl, drop=True)\n",
    "\n",
    "# fixed data at the same depth\n",
    "this_var_fixed_onedep = this_var_fix[0,:]\n",
    "\n",
    "\n",
    "# # deepest\n",
    "# zpl = np.min(sdata['dep'].data)\n",
    "# this_var_onedep = sdata[vbl].where(sdata.dep == zpl, drop=True)\n",
    "# this_var_reshaped_onedep = this_var_reshape[494,:]\n",
    "# print(zpl)\n",
    "\n",
    "\n",
    "tpl = np.linspace(0,np.max(sdata['time']),len(this_var_reshaped_onedep))\n",
    "\n",
    "\n",
    "plt.plot(sdata['time'].where(sdata.dep == zpl, drop=True).data, this_var_onedep.data,'.')\n",
    "plt.plot(tpl,this_var_fixed_onedep,'r-.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b5ec37c-545b-4790-878c-002cace50e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSSE_ACC_SMST_sim_uctd_2012-01-01_to_2012-02-01\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# USER INPUTS:\n",
    "# --------------------------------------------------------------------\n",
    "import datetime\n",
    "# specify region from this list:\n",
    "# WesternMed  ROAM_MIZ  NewCaledonia  NWPacific  BassStrait  RockallTrough  ACC_SMST\n",
    "# MarmaraSea  LabradorSea  CapeBasin\n",
    "RegionName = 'ACC_SMST' \n",
    "\n",
    "# specify date range as start date & number of days.\n",
    "start_date = datetime.date(2012,1,1)\n",
    "ndays = 31\n",
    "# ndays = 2\n",
    "# ndays = 10\n",
    "\n",
    "# directory where data files are stored\n",
    "# datadir = './data/' # default is ./data\n",
    "datadir = '/data1/adac/mitgcm/netcdf/' + RegionName + '/'\n",
    "outputdir = '/data1/adac/osse_output/' + RegionName + '/'\n",
    "argodir = '/data1/argo/gridded/' # store argo climatology here\n",
    "\n",
    "# optional details for sampling (if not specified, reasonable defaults will be used)\n",
    "sampling_details = {\n",
    "    'SAMPLING_STRATEGY' : 'sim_uctd', # options: sim_glider, sim_uctd or trajectory_file.add: mooring, ASV\n",
    "#     'SAMPLING_STRATEGY' : 'sim_mooring', # options: sim_glider, sim_uctd or trajectory_file.add: mooring, ASV\n",
    "    'PATTERN' : 'lawnmower', # back-forth or lawnmower \n",
    "    'zrange' : [-1, -1000],  # depth range of T/S profiles (down is negative). * add U/V range? *\n",
    "    #'hspeed' : 0.25,  # platform horizontal speed in m/s\n",
    "    #'vspeed' : 0.1, # platform vertical (profile) speed in m/s \n",
    "    #'trajectory_file' : None, # if SAMPLING_STRATEGY = 'trajectory_file', specify trajectory file\n",
    "    #'AT_END' : 'repeat' # behaviour at and of trajectory: 'reverse', 'repeat' or 'terminate'. (could also 'restart'?)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sampling_details['SAMPLING_STRATEGY']\n",
    "end_date = start_date + datetime.timedelta(days=ndays)\n",
    " \n",
    "filename_out = (f'OSSE_{RegionName}_{sampling_details[\"SAMPLING_STRATEGY\"]}_{start_date}_to_{end_date}')\n",
    "# filename_out = (f'OSSE_{RegionName}_{sampling_details[\"SAMPLING_STRATEGY\"]}')\n",
    "\n",
    "print(filename_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063e6f5-9670-45b8-9a45-287b8ca16535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db1d3c-89b8-4236-9e1e-b83636a52ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38320a83-4ab0-4b43-91b1-33523b1b42a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c46e3-27ca-4d17-bf71-056bf70d931b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3a05bc-1ec2-4d2c-b105-9d1cb66c8c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c475e8-1a8d-43f1-8bc4-7401215451ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5253e-01fc-4ae8-89ae-7f5f5a6ba70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e9f4c6-bc17-44cc-8cd6-76518f83be54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f47aa-d279-4247-9a39-2f9efad12e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17b3ec-73a5-4f90-8c33-4e9dd5318353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c6ead-be14-4f40-bf9f-420f6084540b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd167b5-b639-4aac-9b07-572fe2d0a630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38c4cedf-71bb-4f41-aef6-88d10f453512",
   "metadata": {},
   "source": [
    "#### EXPLORE DIFFERENT INTERPOLATION METHODS \n",
    "\n",
    "It's re-gridding the 3-d interpolated data that is slow. (2d is trivial)\n",
    "\n",
    "Which part?\n",
    "the line this_var = subsampled_data[vbl].data.compute().copy() \n",
    "is SUPER slow (20 sec) for V, slow (9 sec) for U, moderate (2 sec) for theta and salt, 0 for SH and vorticity\n",
    "\n",
    "I guess because making those regridding computations is slow?\n",
    "- is the regridding necessary? (yes, to do it right - but could check)\n",
    "- can we regrid when we derive fields? (yes - but not sure it's any faster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fa946-00cb-408a-b9c9-3222bef681dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL CODE in osse_tools.py\n",
    "start_time = time.time()\n",
    "# vbls3d = ['U','V', 'Theta','Salt','vorticity','steric_height']\n",
    "# vbls2d = ['steric_height_true', 'Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "\n",
    "\n",
    "vbls3d = ['U_c','V_c', 'U', 'V']\n",
    "vbls3d = ['U', 'V']\n",
    "\n",
    "# -----\n",
    "\n",
    "# ------Regrid the data to depth/time (3-d fields) or subsample to time (2-d fields)\n",
    "print('Gridding the interpolated data...')\n",
    "# get times associated with profiles:\n",
    "SAMPLING_STRATEGY = survey_track['SAMPLING_STRATEGY']\n",
    "if SAMPLING_STRATEGY == 'sim_mooring':\n",
    "    # - for mooring, use the subsampled time grid:\n",
    "    times = np.unique(subsampled_data.time.values)\n",
    "else:\n",
    "    # -- for glider/uctd, take the shallowest & deepest profiles (every second value, since top/bottom get sampled twice for each profile)\n",
    "    time_deepest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True).values[0:-1:2]\n",
    "    time_shallowest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.max(), drop=True).values[0:-1:2]\n",
    "    times = np.sort(np.concatenate((time_shallowest, time_deepest)))\n",
    "    # this results in a time grid that may not be uniformly spaced, but is correct\n",
    "    # - for a uniform grid, use the mean time spacing - may not be perfectly accurate, but is evenly spaced\n",
    "    dt = np.mean(np.diff(time_shallowest))/2 # average spacing of profiles (half of one up/down, so divide by two)\n",
    "    times_uniform = np.arange(survey_track.n_profiles.values*2) * dt\n",
    "    print(\"--- deal with times: %s seconds ---\" % int(time.time() - start_time))\n",
    "# nt is the number of profiles (times):\n",
    "nt = len(times)  \n",
    "# xgr is the vertical grid; nz is the number of depths for each profile\n",
    "# depths are negative, so sort in reverse order using flip\n",
    "zgridded = np.flip(np.unique(subsampled_data.dep.data))\n",
    "nz = int(len(zgridded))\n",
    "\n",
    "# -- initialize the dataset:\n",
    "sgridded = xr.Dataset(\n",
    "    coords = dict(depth=([\"depth\"],zgridded),\n",
    "              time=([\"time\"],times))\n",
    ")\n",
    "# -- 3-d fields: loop & reshape 3-d data from profiles to a 2-d (depth-time) grid:\n",
    "# first, extract each variable, then reshape to a grid\n",
    "# add U and V to the list:\n",
    "# vbls3d.append('U')\n",
    "# vbls3d.append('V')\n",
    "\n",
    "\n",
    "for vbl in vbls3d:\n",
    "    print(vbl)\n",
    "    start_time = time.time()\n",
    "    this_var = subsampled_data[vbl].data.compute().copy() \n",
    "    print(\"--- this_var = subsampled_data[vbl].data.compute().copy(): %s seconds ---\" % int(time.time() - start_time))\n",
    "\n",
    "    # reshape to nz,nt\n",
    "    start_time = time.time()\n",
    "    this_var_reshape = np.reshape(this_var,(nz,nt), order='F') # fortran order is important!\n",
    "    print(\"--- this_var_reshape = np.reshape(this_var,(nz,nt), order='F'): %s seconds ---\" % int(time.time() - start_time))\n",
    "    # for platforms with up & down profiles (uCTD and glider),\n",
    "    # every second column is upside-down (upcast data)\n",
    "    # starting with the first column, flip the data upside down so that upcasts go from top to bottom\n",
    "    start_time = time.time()\n",
    "    if SAMPLING_STRATEGY != 'sim_mooring':\n",
    "        this_var_fix = this_var_reshape.copy()\n",
    "        #this_var_fix[:,0::2] = this_var_fix[-1::-1,0::2] \n",
    "        this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]  # Starting with SECOND column\n",
    "        sgridded[vbl] = ((\"depth\",\"time\"), this_var_fix)\n",
    "    elif SAMPLING_STRATEGY == 'sim_mooring':\n",
    "        sgridded[vbl] = ((\"depth\",\"time\"), this_var_reshape)\n",
    "        \n",
    "    print(\"--- this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]: %s seconds ---\" % int(time.time() - start_time))\n",
    "print(\"--- 3d vars: TOTAL %s seconds ---\" % int(time.time() - start_time))\n",
    "\n",
    "# # for sampled steric height, we want the value integrated from the deepest sampling depth:\n",
    "# sgridded['steric_height'] = ((\"time\"), sgridded['steric_height'].isel(depth=nz-1))\n",
    "# # rename to \"sampled\" for clarity\n",
    "# sgridded.rename_vars({'steric_height':'steric_height_sampled'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b00f52-1bfd-4aa4-a99c-078fa9abe585",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be0213-f605-4244-9159-6e63a94cafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataset to contain the interpolated data, and interpolate\n",
    "subsampled_data = xr.Dataset(\n",
    "    dict(\n",
    "        t = xr.DataArray(survey_track.time, dims='points'), # call this time, for now, so that the interpolation works\n",
    "        lon = xr.DataArray(survey_track.lon, dims='points'),\n",
    "        lat = xr.DataArray(survey_track.lat, dims='points'),\n",
    "        dep = xr.DataArray(survey_track.dep, dims='points'),\n",
    "        points = xr.DataArray(survey_track.points, dims='points')\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Interpolating model fields to the sampling track...')\n",
    "# loop & interpolate through 3d variables:\n",
    "vbls3d = ['Theta','Salt','vorticity','steric_height']\n",
    "for vbl in vbls3d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "# Interpolate U and V from i_g, j_g to i, j, then interpolate:\n",
    "# Get u, v\n",
    "grid = Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "                        'Y':{'center': 'j', 'left': 'j_g'},\n",
    "                        'Z':{'center': 'k'}})\n",
    "U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "subsampled_data['U'] = U_c.interp(survey_indices)\n",
    "subsampled_data['V'] = V_c.interp(survey_indices)    \n",
    "# # subsampled_data['U_test']=ds['U'].interp(survey_indices)\n",
    "# # ds\n",
    "# survey_indices\n",
    "# grid = Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "#                         'Y':{'center': 'j', 'left': 'j_g'},\n",
    "#                         'Z':{'center': 'k'}})\n",
    "# U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "# V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "subsampled_data['U_test']=ds['U'].interp(survey_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0fa5a-15e5-4a76-b3e1-e080c6cef47c",
   "metadata": {},
   "source": [
    "### TEST! is it faster to interpolate mooring data to a grid rather than as \"points\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcfa97-e728-41de-894b-922dadca9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb270e-d94a-4072-9a2f-0effe3554427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate to xi,yi,ti,zi\n",
    "xi = sampling_parameters['xmooring']\n",
    "yi = sampling_parameters['ymooring']\n",
    "ti = ds['time']\n",
    "zi = sampling_parameters['zmooring_TS']\n",
    "\n",
    "# ---- copied ----\n",
    "# Convert lon, lat and z to index i, j and k with f_x, f_y and f_z\n",
    "# XC, YC and Z are the same at all times, so select a single time\n",
    "X = ds.XC.isel(time=0) \n",
    "Y = ds.YC.isel(time=0)\n",
    "i = ds.i\n",
    "j = ds.j\n",
    "z = ds.Z.isel(time=0)\n",
    "k = ds.k\n",
    "f_x = interpolate.interp1d(X[0,:].values, i)\n",
    "f_y = interpolate.interp1d(Y[:,0].values, j)\n",
    "f_z = interpolate.interp1d(z, k, bounds_error=False)\n",
    "# ---- copied ----\n",
    "    \n",
    "# survey_indices_mooring= xr.Dataset(\n",
    "#         dict(\n",
    "#             i = xr.DataArray(f_x(xmooring), dims='points'),\n",
    "#             j = xr.DataArray(f_y(ymooring), dims='points'),\n",
    "#             k = xr.DataArray(f_z(survey_track.dep), dims='points'),\n",
    "#             time = xr.DataArray(survey_track.time, dims='points'),\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50f781-dfa2-4c5f-b5b3-803b495d1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- initialize the dataset:\n",
    "# mgridded = xr.Dataset(\n",
    "#     coords = dict(depth=([\"depth\"],zgridded),\n",
    "#               time=([\"time\"],times))\n",
    "# # )\n",
    "# vbls3d = ['Theta','Salt','vorticity','steric_height', 'U', 'V']\n",
    "# vbls3d = ['Theta','Salt','vorticity','steric_height', 'U_c', 'V_c']\n",
    "# for vbl in vbls3d:\n",
    "#     print('interpolating ' + vbl)\n",
    "#     mgridded[vbl]=ds[vbl].interp(i=f_x(xi), j=f_y(yi), k=f_z(zi), time=ti).compute()\n",
    "   \n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25501083-4690-499a-8a7a-c1cb52540924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630646b-00c2-4eaa-8388-449b91567b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(np.transpose(dum.data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5d545-0b7e-42c8-9afa-56e99ccd1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgridded['Theta'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e13e7-cc1f-45e3-af52-8e36799d3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try regridding U/V earlier (when deriving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac11dd-cbe9-44ba-a6cf-8dffb12743bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgridded.oceTAUX.plot()\n",
    "plt.quiver(sgridded.time.data,0,sgridded.oceTAUX.data, sgridded.oceTAUY.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dfc9a-cf2f-4ba8-a6c2-4deb58b30879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102b0f5-92a8-4bf5-ae10-c9d8152cf0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80809e8-c152-4116-85de-4c0e0b9e98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to compare the two methods \n",
    "# ( not quite working )\n",
    "tpl = ds['time']/24,sss[:,0,0,0]\n",
    "ttt=ds['time']\n",
    "# ipl = ( subsampled_data.dep.values == subsampled_data.dep.values.min() )\n",
    "# plt.plot(subsampled_data.time(ipl),subsampled_data.Salt.sel(ipl).data,'-')\n",
    "# plt.plot(subsampled_data.time,subsampled_data.Salt.data,'-')\n",
    "plt.plot(tgr,np.transpose(dum),'-')\n",
    "plt.plot(ttt[:],sss[:,0,0,0],'k--')\n",
    "# plt.legend('survey_interp method','direct interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b425ee-04b8-44b9-8c6a-e4d545914ba3",
   "metadata": {},
   "source": [
    "### Visualize steric height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6e68c-0d21-48ec-b426-d409346c6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sh_anom = sh_true.values - sh_true.values.mean()\n",
    "plt.figure(figsize=(7,5))\n",
    "sho = plt.scatter(survey_track.lon, survey_track.lat, c=sh_anom)\n",
    "plt.title('Steric height anomaly')\n",
    "plt.colorbar(sho).set_label('m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bee052-23a5-47f8-b910-f2e3d61ac20f",
   "metadata": {},
   "source": [
    "### Comparison of true vs sampled steric height\n",
    "Plot comparing the \"true\" steric height along the track and the subsampled steric height, which is computed by integrating the specific volume anomaly for each subsampled profile from its deepest sampling depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c871c-16ea-43b1-8d97-a60523ac5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# truth:\n",
    "plt.plot(sh_true.time, sh_true.values - sh_true.values.mean())\n",
    "# get index of the deepest sampling depths\n",
    "i = ( subsampled_data.dep.values == subsampled_data.dep.values.min() )\n",
    "plt.plot(subsampled_data.time.values[i], subsampled_data.steric_height.values[i] - subsampled_data.steric_height.values[i].mean(),'.-')\n",
    "plt.title('Steric height anomaly along the survey track')\n",
    "plt.legend(['truth','subsampled data'])\n",
    "plt.xlabel('time, days')\n",
    "plt.ylabel('steric height anom., m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ba6b4-6ab4-486b-8df3-252a753dada2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3cdad-01aa-4316-9f31-daf8927d902f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb543739-0ada-4a79-a79b-3f05d0786a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaeac36-0fc9-48cb-bb5d-452b0413ef42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b3ce9-027b-4c2c-98cf-6e5edbd7dd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be9304-8094-4f85-9269-4150b11c4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE  BELOW\n",
    "\n",
    "# interpolation:\n",
    "subsampled_data = xr.Dataset()  \n",
    "\n",
    "# loop & interpolate through 3d variables:\n",
    "vbls3d = ['Theta','Salt','vorticity']\n",
    "# vbls3d = ['Theta']\n",
    "for vbl in vbls3d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "# # Interpolate U and V from i_g, j_g to i, j, then interpolate:\n",
    "# U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "# V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "# subsampled_data['U'] = U_c.interp(survey_indices)\n",
    "# subsampled_data['V'] = V_c.interp(survey_indices)\n",
    "\n",
    "subsampled_data['lon']=survey_track.lon\n",
    "subsampled_data['lat']=survey_track.lat\n",
    "subsampled_data['dep']=survey_track.dep\n",
    "subsampled_data['time']=survey_track.time \n",
    "\n",
    "# loop & interpolate through 2d variables:\n",
    "vbls2d = ['Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux', 'oceTAUX', 'oceTAUY']\n",
    "vbls2d = ['Eta', 'Depth']\n",
    "# create 2-d survey track by removing the depth dimension\n",
    "survey_indices_2d =  survey_indices.drop_vars('k')\n",
    "for vbl in vbls2d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices_2d)\n",
    "# survey_indices_2d.i.plot()\n",
    "# ds['KPPhbl'].interp(survey_indices_2d).plot()\n",
    "\n",
    "# interp\n",
    "# this returns a value at every timestep (points) - very high resolution\n",
    "# - could subsample to the model time grid \n",
    "\n",
    "\n",
    "# plot\n",
    "# plt.plot(sh_true.time, sh_true.values - sh_true.values.mean())\n",
    "# plt.plot(subsampled_data.time, ssh - ssh.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c03be6d-bf34-4606-9550-b12520654b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data = xr.Dataset() \n",
    "\n",
    "# loop & interpolate through 3d variables:\n",
    "vbls3d = ['Theta','Salt','vorticity','steric_height']\n",
    "for vbl in vbls3d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "# Interpolate U and V from i_g, j_g to i, j, then interpolate:\n",
    "# Get u, v\n",
    "grid = Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "                        'Y':{'center': 'j', 'left': 'j_g'},\n",
    "                        'Z':{'center': 'k'}})\n",
    "U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "subsampled_data['U'] = U_c.interp(survey_indices)\n",
    "subsampled_data['V'] = V_c.interp(survey_indices)\n",
    "\n",
    "\n",
    "# add lat/lon/time to dataset\n",
    "subsampled_data['lon']=survey_track.lon\n",
    "subsampled_data['lat']=survey_track.lat\n",
    "subsampled_data['dep']=survey_track.dep\n",
    "subsampled_data['time']=survey_track.time  \n",
    "\n",
    "# loop & interpolate through 2d variables:\n",
    "vbls2d = ['Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "# create 2-d survey track by removing the depth dimension\n",
    "survey_indices_2d =  survey_indices.drop_vars('k')\n",
    "for vbl in vbls2d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices_2d)   \n",
    "# taux & tauy must be treated like U and V\n",
    "oceTAUX_c = grid.interp(ds.oceTAUX, 'X', boundary='extend')\n",
    "oceTAUY_c = grid.interp(ds.oceTAUY, 'Y', boundary='extend')\n",
    "subsampled_data['oceTAUX'] = oceTAUX_c.interp(survey_indices_2d)\n",
    "subsampled_data['oceTAUY'] = oceTAUY_c.interp(survey_indices_2d)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3fd6e-8149-4892-bc03-9f0fb9a3ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661ee6a-a032-42d2-b75a-f115ec88a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sh_true.time, sh_true.values - sh_true.values.mean())\n",
    "plt.plot(subsampled_data.time, ssh - ssh.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf5f6b-f1f2-4e68-a593-85679e04ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mooring test\n",
    "\n",
    "model_xav = ds.XC.cccc.mean(dim='i').values\n",
    "model_yav = ds.YC.isel(time=0, i=0).mean(dim='j').values\n",
    "\n",
    "xmooring = model_xav # default lat/lon is the center of the domain\n",
    "ymooring = model_yav\n",
    "zmooring_TS = [-1, -10, -50, -100] # depth of T/S instruments\n",
    "zmooring_UV = [-1, -10, -50, -100, -200, -300, -400, -500] # depth of U/V instruments\n",
    "\n",
    "\n",
    "ts = ds.time\n",
    "n_samples = ts.size\n",
    "n_depths_TS = np.size(zmooring_TS)\n",
    "n_depths_UV = np.size(zmooring_UV)\n",
    "# depth sampling - different for TS and UV\n",
    "zs_TS = np.transpose(np.tile(zmooring_TS,(n_samples,1)))\n",
    "zs_UV = np.transpose(np.tile(zmooring_UV,(n_samples,1)))\n",
    "xs_TS = xmooring * np.ones([n_depths_TS, n_samples])\n",
    "ys_TS = ymooring * np.ones([n_depths_TS, n_samples])\n",
    "xs_UV = xmooring * np.ones([n_depths_TS, n_samples])\n",
    "ys_UV = ymooring * np.ones([n_depths_TS, n_samples])\n",
    "\n",
    "survey_track = xr.Dataset(\n",
    "    dict(\n",
    "        lon = xr.DataArray(xs_TS,dims='points'),\n",
    "        lat = xr.DataArray(ys_TS,dims='points'),\n",
    "        dep = xr.DataArray(zs_TS,dims='points'),\n",
    "        time = xr.DataArray(ts_TS,dims='points')\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492a034-f24a-45d2-b11c-2009ff1b725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.broadcast_to([-1, -10, -50, -100],(2,4))\n",
    "np.broadcast_to([-1, -10, -50, -100],(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d3306-3698-49a8-9938-4ba6867fae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.broadcast_to(zmooring_TS,(2,4))\n",
    "# np.tile(zmooring_TS.transpose,(2,1))\n",
    "# np.tile(np.transpose(zmooring_TS),(1,n_samples)).shape\n",
    "# xs_TS.shape\n",
    "# [-1, -10, -50, -100]\n",
    "# zmooring_TS\n",
    "\n",
    "# np.transpose(np.array(zmooring_TS)).shape \n",
    "np.transpose(np.tile(zmooring_TS,(n_samples,1))).shape\n",
    "xs_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6331b-178e-437f-87c6-ae42686529d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ff611-de21-47ea-be4f-0103c61d4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(subsampled_data.time,subsampled_data.dep,c=subsampled_data.Theta)\n",
    "plt.plot(subsampled_data.time,-subsampled_data.Depth,c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574b4d3-0344-4fb8-a65f-66424cd9b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_true.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc15fb-4d21-40f1-91cb-531ddc392a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we interpolated everything to the \"points\" - one datapoint per sample\n",
    "# but we may also want a cleaner (gridded) output product that has been reshaped into \n",
    "# profiles (i.e., a X x Z x T array)\n",
    "# (This might not make sense for glider data, since profiles are likely > one model gridpoint)\n",
    "\n",
    "# separate these into \"subsampled_points\" and \"subsampled_profiles\"?\n",
    "\n",
    "# a couple ways to determine the profiles:\n",
    "# - one per down / one per up, with time as either the start/mean/end\n",
    "# - one per down-up, with the time as the deepest point\n",
    "# - maybe others? would be useful to get feedback on this...\n",
    "\n",
    "# use \"where\" to determine the indices of the start/end (shallowest/deepest) of each profile:\n",
    "\n",
    "# subsampled_data.steric_height.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True)\n",
    "# subsampled_deepest = subsampled_data.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True)\n",
    "\n",
    "# plt.plot(subsampled_data.dep.values[ideep])\n",
    "\n",
    "# this is the DEEPEST point only. \n",
    "subsampled_data = subsampled_data\n",
    "dum = subsampled_data.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True)\n",
    "shall = subsampled_data.where(subsampled_data.dep == subsampled_data.dep.max(), drop=True)\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(dum.lon,dum.lat,'.')\n",
    "plt.plot(shall.lon,shall.lat,'.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a03dd4-2778-4294-b5ab-c66753cdca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# index?\n",
    "ishallow = ( subsampled_data.dep.values == subsampled_data.dep.values.max() )\n",
    "ideep = ( subsampled_data.dep.values == subsampled_data.dep.values.min() )\n",
    "# boolean to index\n",
    "ishallow = [ishallow for ishallow, x in enumerate(ishallow) if x]\n",
    "ideep = [ideep for ideep, x in enumerate(ideep) if x]\n",
    "\n",
    "t_profiles = subsampled_data['time'].isel(points=ishallow)\n",
    "t_profiles.plot()\n",
    "z = np.unique(subsampled_data['dep'])\n",
    "\n",
    "# # # initialize the dataset:\n",
    "# subsampled_profile = xr.Dataset(\n",
    "#     coords={\n",
    "#         \"time\": t_profiles,\n",
    "#         \"depth\": z\n",
    "#     },\n",
    "#     \"Salt\": ((\"time\", \"depth\"), []),\n",
    "# )\n",
    "\n",
    "\n",
    "# loop through each profile:\n",
    "\n",
    "pr = []\n",
    "for n in np.arange(np.size(ishallow)):\n",
    "    i = np.arange(ishallow[n],ideep[n])\n",
    "    # append\n",
    "    pr = np.append(pr,subsampled_data['Salt'].isel(points=i))\n",
    "    \n",
    "#     # loop & interpolate through 3d variables:\n",
    "#     for vbl in vbls3d:\n",
    "# #         subsampled_profile[vbl] = subsampled_data[vbl].isel(points=i)\n",
    "#         dum = subsampled_data[vbl].isel(points=i)\n",
    "\n",
    "pr   \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a73cad-43d7-47eb-be88-d8e72f92ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120d7a8-bfde-4caf-ad80-e9bcf1713559",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fd255-3e82-4f9b-bae3-0ec449c9ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # n=0\n",
    "# # i = np.arange(ishallow[n],ideep[n])\n",
    "# # start = time.time()\n",
    "# # dum = subsampled_data.Theta.isel(points=i)\n",
    "# # end = time.time()\n",
    "# # print(\"The time of execution of above program is :\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04775a5-abb2-4353-a758-777169f13982",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data.groupby(\"dep\").mean().scatter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1935d0-8aa0-449a-ba7d-125dbd1c899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = xr.Dataset(\n",
    "    {\n",
    "        \"Salt\": ((\"time\", \"depth\"), [subsampled_data.Salt.isel(points=i)]),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": [1],\n",
    "        \"depth\": z,\n",
    "    },\n",
    "    \n",
    ")\n",
    "ss\n",
    "# \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3143cd-f8c1-4c8a-9049-bb56ca2b09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0125ef3-56b1-4930-b775-a3f6b08a8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46c19d-bc90-46fe-ba47-41851dcc0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {**sampling_details}\n",
    "\n",
    "\n",
    "def test_code(sampling_details):\n",
    "    for key, value in sampling_details.items():\n",
    "#         print(key , '=' , value , '')\n",
    "#         print(type(key))\n",
    "        if isinstance(value,str):\n",
    "            exec(key + '=\"' + value + '\"',None, globals())\n",
    "#             print(key)\n",
    "            \n",
    "        if isinstance(value,list):\n",
    "            exec(key + '=' + str(value) + '',locals(), globals())\n",
    "#             print(key)\n",
    "            \n",
    "    print(zrange+1000)\n",
    "    return zrange\n",
    "    \n",
    "ddd = test_code(sampling_details)\n",
    "# test_code(sampling_details)\n",
    "ddd\n",
    "# for key in sampling_details:\n",
    "#     exec(key + '=' + '\"' + sampling_details[key] + '\"')\n",
    "\n",
    "# isinstance(sampling_details['zrange'],list)\n",
    "# isinstance(sampling_details['SAMPLING_STRATEGY'],list)\n",
    "# isinstance?\n",
    "\n",
    "# type(sampling_details['zrange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6797d2a-5c8c-4c83-b77b-2f616526695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#             # 3) merge transformed data with ds\n",
    "#             print('merging with ds')\n",
    "#             print(ds)\n",
    "#             print(oceTAUX_r)\n",
    "#             ds = ds.merge(U_r.to_dataset(name='U_r')).merge(V_r.to_dataset(name='V_r'))\n",
    "#             ds = ds.merge(oceTAUX_r.to_dataset(name='oceTAUX_r')).merge(oceTAUY_r.to_dataset(name='oceTAUY_r'))\n",
    "\n",
    "#             # 4) rename transformed data to original names (renaming to 'temp' first)\n",
    "#             # !! caution, skipping this step will cause great confusion later !!\n",
    "#             ds = ds.rename_vars({'U':'Utemp', 'V':'Vtemp'}).rename_vars({'U_r':'U', 'V_r':'V'})\n",
    "#             ds = ds.rename_vars({'oceTAUX':'TAUXtemp', 'oceTAUY':'TAUYtemp'}).rename_vars({'oceTAUX_r':'oceTAUX', 'oceTAUY_r':'oceTAUY'})\n",
    "           \n",
    "\n",
    "#             # 6) get rid of all the original U/V/TAUX/TAUY variables (which have been renamed to *temp)\n",
    "#             ds = ds.drop_vars({'Utemp','Vtemp','TAUXtemp','TAUYtemp'})\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f50e8b-2cb2-44e8-bea5-5bf0ac0685d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next two cells: explore looping through files & using concat instead of opening w open_mfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cc82c-c027-4d91-8fc6-757f58e68757",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# or - try loading in a loop and using concat:\n",
    "ds2 = xr.open_dataset(target_files[0], drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'},\n",
    "                     chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "for fn in target_files[1:-1]:\n",
    "    thisds = xr.open_dataset(fn, drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'},\n",
    "                            chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "    ds2 = xr.concat([ds2, thisds], dim='time')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d45b9-f240-4992-beee-7881011e7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#ok, so what if we load/concat target_files and derived_files - which crashes if we use open_mfdataset\n",
    "\n",
    "# load the corresponding derived fields (includes steric height, vorticity, and transformed vector variables for current and wind stress)\n",
    "derivedir = datadir + 'derived/'\n",
    "derived_files = [f'{derivedir}LLC4320_pre-SWOT_{RegionName}_derived-fields_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "\n",
    "ds_all = xr.open_dataset(target_files[0], drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'},\n",
    "                     chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "dsd = xr.open_dataset(derived_files[0], chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "ds_all = ds_all.merge(dsd)\n",
    "n = 0\n",
    "for fn in target_files[1:-1]:\n",
    "    # original data:\n",
    "    thisds = xr.open_dataset(fn, drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'},\n",
    "                            chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "    # derived file. \n",
    "    n+=1\n",
    "    thisdsd = xr.open_dataset(derived_files[n], chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "    # merge the two:\n",
    "    thisds = thisds.merge(thisdsd)\n",
    "    # and concat with all files\n",
    "    ds_all = xr.concat([ds_all, thisds], dim='time')\n",
    "# rename the transformed vector variables to their original names\n",
    "ds_all = ds_all.rename_vars({'U_transformed':'U', 'V_transformed':'V',\n",
    "                             'oceTAUX_transformed':'oceTAUX', 'oceTAUY_transformed':'oceTAUY'})\n",
    "# Convert lon, lat and z to index i, j and k with f_x, f_y and f_z\n",
    "# XC, YC and Z are the same at all times, so select a single time\n",
    "X = ds_all.XC.isel(time=0) \n",
    "Y = ds_all.YC.isel(time=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf057b7-6cd4-4402-b5ac-d8a8bdcf4f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d43811-aa9f-472d-8e63-ec3096d6cc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a7479-0aee-4230-a6a7-0da6d6501a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4525e6c-1198-4d80-b648-38eb8e87f266",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test using xoak:\n",
    "This seems to give approx equivalent results as my method, but I can't figure out how to optimize the parameters so that it doesn't crash the kernel ... so I will put this aside for now & stick with the orginal method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd453705-0443-46f4-96c4-84ad088b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# have to create a new dataset to do use xoak, because the coords have to have the same dims as the variables\n",
    "\n",
    "\n",
    "# time has to be a float\n",
    "ds['timef'] = ds.time.copy()  # in datetime64 format\n",
    "# converting date time into second timestamp \n",
    "tt = ds.timef\n",
    "tt = (tt - tt[0]) / np.timedelta64(1, 'h')\n",
    "ds = ds.assign_coords(timef=tt)\n",
    "\n",
    "# create coords that all have the same shape as the data:\n",
    "# for 3d variables (T x Z x Y x X)\n",
    "X3d = ds.XC.broadcast_like(ds.Z)\n",
    "Y3d = ds.YC.broadcast_like(ds.Z)\n",
    "T3d = ds.timef.broadcast_like(X3d)\n",
    "Z3d = ds.Z.broadcast_like(X3d)\n",
    "# for 2d variables (T x Z x Y)\n",
    "X2d = ds.XC # already has the right dimensions\n",
    "Y2d = ds.YC # already has the right dimensions\n",
    "T2d = ds.timef.broadcast_like(X2d)\n",
    "\n",
    "# # => to use dask, chunk the data.\n",
    "# chunk_shape = (ds.Theta.chunks[0][0], ds.Theta.chunks[1][0], ds.Theta.chunks[2][0], ds.Theta.chunks[3][0])\n",
    "\n",
    "# create a new dataset with the variables to interpolate and the 4-d coordinates:\n",
    "dsoak_3d = xr.Dataset(\n",
    "    coords={'X': (('time', 'k', 'j', 'i'), X3d.values), 'Y': (('time', 'k', 'j', 'i'), Y3d.values), \n",
    "            'T': (('time', 'k', 'j', 'i'), T3d.values), 'Z': (('time', 'k', 'j', 'i'), Z3d.values)},\n",
    "    data_vars={'Theta': (('time', 'k', 'j', 'i'), ds.Theta.data)},\n",
    ")\n",
    "dsoak_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b17fa8-b9d3-4327-8285-59fff20e2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rechunk (LEADS TO AN ERROR IN xoak.set_index)\n",
    "# dsoak_3d = dsoak_3d.chunk({\"time\": tchunk, \"j\":ychunk, \"i\":xchunk})\n",
    "# dsoak_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e078f09-a96d-4890-8851-6514854dd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for 2d:\n",
    "# create a new dataset with the variables to interpolate and the 4-d coordinates:\n",
    "dsoak_2d = xr.Dataset(\n",
    "    coords={'X': (('time', 'j', 'i'), X2d.values), 'Y': (('time', 'j', 'i'), Y2d.values), \n",
    "            'T': (('time', 'j', 'i'), T2d.values)},\n",
    "    data_vars={'Eta': (('time', 'j', 'i'), ds.Eta.data)},\n",
    ")\n",
    "dsoak_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c0e8d-26cd-445b-868c-182ce4df026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# xoak: set index\n",
    "# options are:\n",
    "#     scipy_kdtree           --  fails with an error\n",
    "#     sklearn_kdtree         --  kernel crashes \n",
    "#     sklearn_balltree       --  this is the only one that works, but nearest neighbor - coarse and not accurate?\n",
    "#     sklearn_geo_balltree   --  doesn't work except with lat/lon\n",
    "#     s2point                --  seems to give a single point at the start/end!\n",
    "#     brute_force            --  kernel crashes\n",
    "\n",
    "dsoak_3d.xoak.set_index(['X', 'Y', 'T', 'Z'], 'sklearn_balltree') \n",
    "# should have one object per chunk, but there's only one object so I guess it only created one chunk\n",
    "# however, if I rechunk earlier it gives an error at this step.\n",
    "dsoak_3d.xoak.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4920a-e0df-4403-b61d-e18b7d840e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert time in survey_track hours (since start of the time in dsoak_3d)\n",
    "\n",
    "# time has to be a float\n",
    "survey_track['timef'] = survey_track.time.copy()  # in datetime64 format\n",
    "# converting date time into second timestamp \n",
    "ttrack = survey_track.timef\n",
    "ttrack = (ttrack - ds.time.data[0]) / np.timedelta64(1, 'h') # ref time time[0] for the dataset\n",
    "survey_track = survey_track.assign_coords(timef=ttrack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc23bc6-573c-45d0-882c-6d0f9b6a3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can chunk survey_track\n",
    "# - this seems to improve performance but also leads to a kernel restart, so.... maybe I have to play with the params\n",
    "# survey_track\n",
    "# survey_track = survey_track.chunk({\"points\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4b9e8-3742-4d93-9907-19c296e35b27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar(), dask.config.set(scheduler='processes'):\n",
    "    subsampled_xoak_3d = dsoak_3d.xoak.sel(\n",
    "        X=survey_track.lon,\n",
    "        Y=survey_track.lat,\n",
    "        T=survey_track.timef,\n",
    "        Z=survey_track.dep\n",
    "    )\n",
    "\n",
    "subsampled_xoak_3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788f585-c5e0-4626-9a7b-9bab897e7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dep, lat, lon, time variables to be consistent with subsampled_data:\n",
    "subsampled_xoak_3d['dep'] = subsampled_xoak_3d.Z\n",
    "subsampled_xoak_3d['lat'] = subsampled_xoak_3d.Y\n",
    "subsampled_xoak_3d['lon'] = subsampled_xoak_3d.X\n",
    "subsampled_xoak_3d['time'] = subsampled_xoak_3d.T\n",
    "subsampled_xoak_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fd700-4ffc-49ff-af0a-69d9613d2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data['timef'] = subsampled_data.time.copy()  # in datetime64 format\n",
    "# converting date time into second timestamp \n",
    "ttt = subsampled_data.time\n",
    "ttt = (ttt - ds.time.data[0]) / np.timedelta64(1, 'h') # ref time time[0] for the dataset\n",
    "subsampled_data = subsampled_data.assign_coords(timef=ttt)\n",
    "\n",
    "plt.plot(subsampled_data.timef, subsampled_data.dep,'.')\n",
    "plt.plot(subsampled_xoak_3d.time, subsampled_xoak_3d.dep,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af82e0-d13b-4749-b066-bddf79e16290",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(subsampled_data.Theta,'.')\n",
    "plt.plot(subsampled_xoak_3d.Theta,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9464cc-bad7-4496-a392-ce1577be2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regrid\n",
    "# since we've done a nearest neighbor interp, the data are oversampled\n",
    "# ... can we just reshape?\n",
    "# yes, but the grid can't be linear\n",
    "# *and* it can't be unique\n",
    "\n",
    "# => for our example, just figure out length of zgridded by plotting subsampled_xoak_3d['dep']\n",
    "\n",
    "nz = int(1998/2)\n",
    "zgridded = subsampled_xoak_3d['dep'].isel(points=slice(0,nz))\n",
    "\n",
    "# cut length to an integer number of nz:\n",
    "subsampled_xoak_3d = subsampled_xoak_3d.isel(points=slice(0,int(np.floor(subsampled_xoak_3d.points.size/nz)*nz)))\n",
    "times = subsampled_xoak_3d['time'].isel(points=np.arange(0,len(subsampled_xoak_3d.time),nz))\n",
    "nt = len(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488a3c1-4719-422e-ac6e-45ba090f1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- initialize the gridded dataset:\n",
    "sgridded_xoak_3d = xr.Dataset(\n",
    "    coords = dict(depth=([\"depth\"],zgridded.data),\n",
    "              time=([\"time\"],times.data))\n",
    ")\n",
    "# -- 3-d fields: loop & reshape 3-d data from profiles to a 2-d (depth-time) grid:\n",
    "# first, extract each variable, then reshape to a grid\n",
    "vbls3d = ['Theta']\n",
    "\n",
    "SAMPLING_STRATEGY = sampling_parameters['SAMPLING_STRATEGY']\n",
    "\n",
    "for vbl in vbls3d:\n",
    "    print(vbl)\n",
    "    this_var = subsampled_xoak_3d[vbl].data.compute().copy() \n",
    "    # reshape to nz,nt\n",
    "    this_var_reshape = np.reshape(this_var,(nz,nt), order='F') # fortran order is important!\n",
    "    # for platforms with up & down profiles (uCTD and glider),\n",
    "    # every second column is upside-down (upcast data)\n",
    "    # starting with the first column, flip the data upside down so that upcasts go from top to bottom\n",
    "    if SAMPLING_STRATEGY != 'sim_mooring':\n",
    "        this_var_fix = this_var_reshape.copy()\n",
    "        #this_var_fix[:,0::2] = this_var_fix[-1::-1,0::2] \n",
    "        this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]  # Starting with SECOND column\n",
    "        sgridded_xoak_3d[vbl] = ((\"depth\",\"time\"), this_var_fix)\n",
    "    elif SAMPLING_STRATEGY == 'sim_mooring':\n",
    "        sgridded_xoak_3d[vbl] = ((\"depth\",\"time\"), this_var_reshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1315157-aa86-497c-bfbb-589b6c188c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "sgridded_xoak_3d.Theta.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc341c-b5b3-490c-8a49-357a9e87ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original method\n",
    "sgridded.Theta.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671eb91-07f4-47d3-8b57-1d5f0dc7ba79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc35054-f693-4683-84e2-94d81a99813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdca63-cdcd-47c4-8338-b99b4cd4be06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9a75e-6172-4c77-a03c-d60045854db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "317f8935-05a0-47ce-bc34-451fce132e1b",
   "metadata": {},
   "source": [
    "# TESTING!\n",
    "\n",
    "concat() has a number of options which provide deeper control over which variables are concatenated and how it handles conflicting variables between datasets. With the default parameters, xarray will load some coordinate variables into memory to compare them between datasets. This may be prohibitively expensive if you are manipulating your dataset lazily using Parallel computing with Dask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215a957-3448-41c8-962c-06b0c5b7e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # instead of loading all with open_mfdataset, load in a loop?\n",
    "\n",
    "# #ok, so what if we load/concat target_files and derived_files - which crashes if we use open_mfdataset\n",
    "\n",
    "# # load the corresponding derived fields (includes steric height, vorticity, and transformed vector variables for current and wind stress)\n",
    "# derivedir = datadir + 'derived/'\n",
    "# derived_files = [f'{derivedir}LLC4320_pre-SWOT_{RegionName}_derived-fields_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "\n",
    "# ds_all = xr.open_dataset(target_files[0], drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'},\n",
    "#                      chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "# # dsd = xr.open_dataset(derived_files[0], chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "# # ds_all = ds_all.merge(dsd)\n",
    "# n = 0\n",
    "# for fn in target_files[1:-1]:\n",
    "#     # original data:\n",
    "#     thisds = xr.open_dataset(fn, drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'},\n",
    "#                             chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "# #     # derived file. \n",
    "# #     n+=1\n",
    "# #     thisdsd = xr.open_dataset(derived_files[n], chunks={'i':xchunk, 'j':ychunk, 'time':tchunk})\n",
    "# #     # merge the two:\n",
    "# #     thisds = thisds.merge(thisdsd)\n",
    "#     # and concat with all files\n",
    "        \n",
    "#     ds_all = xr.concat([ds_all, thisds], dim='time', \n",
    "#                        data_vars={'XC', 'YC', 'Z', 'Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', \n",
    "#                                   'oceQnet', 'oceQsw', 'oceSflux', 'Theta', 'Salt', 'W'},\n",
    "#                        coords={'time'},\n",
    "#                        compat='override', join='override', combine_attrs='override')\n",
    "\n",
    "# #                        coords={'j_g', 'i', 'i_g', 'j', 'k', 'nb', 'time'}, \n",
    "        \n",
    "# # rename the transformed vector variables to their original names\n",
    "# # ds_all = ds_all.rename_vars({'U_transformed':'U', 'V_transformed':'V',\n",
    "# #                              'oceTAUX_transformed':'oceTAUX', 'oceTAUY_transformed':'oceTAUY'})\n",
    "# # Convert lon, lat and z to index i, j and k with f_x, f_y and f_z\n",
    "# # XC, YC and Z are the same at all times, so select a single time\n",
    "# X = ds_all.XC.isel(time=0) \n",
    "# Y = ds_all.YC.isel(time=0)\n",
    "\n",
    "# ds = ds_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d1b48-9e50-45cb-9fe9-49366976b471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b04359-c86e-43f8-a602-8e40e81cafd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54671ab4-b2f9-4fc3-bbdf-924abcc7eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING !!!\n",
    "vbls3d = ['Theta','Salt','vorticity','steric_height', 'U', 'V']\n",
    "vbls2d = ['steric_height_true', 'Eta', 'KPPhbl', 'PhiBot', 'oceTAUX', 'oceTAUY', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "        \n",
    "vbls3d = ['Theta']\n",
    "  \n",
    "# -- for glider/uctd, take the shallowest & deepest profiles (every second value, since top/bottom get sampled twice for each profile)\n",
    "time_deepest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True).values[0:-1:2]\n",
    "time_shallowest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.max(), drop=True).values[0:-1:2]\n",
    "times = np.sort(np.concatenate((time_shallowest, time_deepest)))\n",
    "# this results in a time grid that may not be uniformly spaced, but is correct\n",
    "# - for a uniform grid, use the mean time spacing - may not be perfectly accurate, but is evenly spaced\n",
    "dt = np.mean(np.diff(time_shallowest))/2 # average spacing of profiles (half of one up/down, so divide by two)\n",
    "times_uniform = np.arange(survey_track.n_profiles.values*2) * dt\n",
    "\n",
    "# nt is the number of profiles (times):\n",
    "nt = len(times)  \n",
    "# xgr is the vertical grid; nz is the number of depths for each profile\n",
    "# depths are negative, so sort in reverse order using flip\n",
    "zgridded = np.flip(np.unique(subsampled_data.dep.data))\n",
    "nz = int(len(zgridded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2a049-2c2d-46a5-b402-5d9cd02216ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# -- initialize the dataset:\n",
    "sgridded = xr.Dataset(\n",
    "    coords = dict(depth=([\"depth\"],zgridded),\n",
    "              time=([\"time\"],times))\n",
    ")\n",
    "# -- 3-d fields: loop & reshape 3-d data from profiles to a 2-d (depth-time) grid:\n",
    "# first, extract each variable, then reshape to a grid\n",
    "vbls3d = ['Theta']\n",
    "for vbl in vbls3d:\n",
    "    print(vbl)\n",
    "    %time this_var = subsampled_data[vbl].data.compute().copy() \n",
    "    # reshape to nz,nt\n",
    "    %time this_var_reshape = np.reshape(this_var,(nz,nt), order='F') # fortran order is important!\n",
    "    # for platforms with up & down profiles (uCTD and glider),\n",
    "    # every second column is upside-down (upcast data)\n",
    "    # starting with the first column, flip the data upside down so that upcasts go from top to bottom\n",
    "    if SAMPLING_STRATEGY != 'sim_mooring':\n",
    "        this_var_fix = this_var_reshape.copy()\n",
    "        #this_var_fix[:,0::2] = this_var_fix[-1::-1,0::2] \n",
    "        this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]  # Starting with SECOND column\n",
    "        sgridded[vbl] = ((\"depth\",\"time\"), this_var_fix)\n",
    "    elif SAMPLING_STRATEGY == 'sim_mooring':\n",
    "        sgridded[vbl] = ((\"depth\",\"time\"), this_var_reshape)\n",
    "\n",
    "\n",
    "# # for sampled steric height, we want the value integrated from the deepest sampling depth:\n",
    "# sgridded['steric_height'] = ((\"time\"), sgridded['steric_height'].isel(depth=nz-1).data)\n",
    "# # rename to \"steric_height_sampled\" for clarity\n",
    "# sgridded.rename_vars({'steric_height':'steric_height_sampled'})\n",
    "\n",
    "\n",
    "\n",
    "# #  -- 2-d fields: loop & reshape 2-d data to the same time grid \n",
    "# for vbl in vbls2d:\n",
    "#     this_var = subsampled_data[vbl].data.compute().copy() \n",
    "#     # subsample to nt\n",
    "#     this_var_sub = this_var[0:-1:nz]\n",
    "#     sgridded[vbl] = ((\"time\"), this_var_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6861ea-a6c9-4b33-b9a8-59c5ece99da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the slow step: subsampled_data[vbl].data.compute().copy()\n",
    "# - can we do this differently?\n",
    "%time this_var = subsampled_data[vbl].data\n",
    "# reshape to nz,nt\n",
    "# %time this_var_reshape = np.reshape(this_var,(nz,nt), order='F') # fortran order is important!\n",
    "%time this_var_reshape = np.reshape(this_var,(nz,nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca139a58-e9b9-4c60-a508-6fcd34a3c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dum = this_var_reshape.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cd348-a3ab-457b-9e20-a2edd87d1a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# end testing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
