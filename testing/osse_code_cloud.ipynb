{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e15606-e20b-45ed-872f-1dac2b874aa6",
   "metadata": {},
   "source": [
    "# CLOUD VERSION \n",
    "\n",
    "\n",
    "ONLY modify the data access parts of the code - ultimately, this will be merged with the latest version of osse_code. There will NOT be two versions.\n",
    "\n",
    "* note- I use a RUN_ON_CLOUD flag; an alternative would be to detect automatically, as is done in the tutorial... https://github.com/podaac/tutorials/blob/master/notebooks/Pre-SWOT_Numerical_Simulation_Demo.ipynb\n",
    "\n",
    "* s3fs.cp (based on the tutorial, which uses s3fs to load directly w xarray) doesn't work  - \"permission denied\" error. So, try another tool?\n",
    "\n",
    "* coudl try s3.download(f, \"DEMO_FILES/\" + os.path.basename(f)) => but is that better for local download?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b986eb-9b7a-4472-8040-ddbfc81e7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "# Native packages\n",
    "from math import radians, degrees, sin, cos, asin, acos, sqrt\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third-party packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Other third-party packages\n",
    "import netCDF4 as nc4\n",
    "\n",
    "# Third-party packages for data interpolation\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import griddata\n",
    "from xgcm import Grid\n",
    "import xgcm.grid\n",
    "\n",
    "# Third-party packages for data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# CLOUD\n",
    "# for running on the AWS cloud:\n",
    "import requests\n",
    "import s3fs\n",
    "\n",
    "\n",
    "# osse tools package\n",
    "# del sys.modules['osse_tools_cloud']  # uncomment if troubleshooting osse_tools\n",
    "from osse_tools_cloud import download_llc4320_data, compute_derived_fields, get_survey_track, survey_interp\n",
    "\n",
    "# dask\n",
    "# from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dab287-3188-426d-866e-89cf3b2e542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure how optimal these parameters are....\n",
    "# client = Client(n_workers=20, threads_per_worker=4, memory_limit='20GB')\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17b87f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# USER INPUTS:\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# flags:\n",
    "RUN_ON_CLOUD = 1 # 0: running locally, downloading data; 1: running on the AWS cloud\n",
    "SAVE_FIGURES = True # True or False\n",
    "\n",
    "\n",
    "# specify region from this list:\n",
    "# WesternMed  ROAM_MIZ  NewCaledonia  NWPacific  BassStrait  RockallTrough  ACC_SMST\n",
    "# MarmaraSea  LabradorSea  CapeBasin\n",
    "RegionName = 'ACC_SMST' \n",
    "\n",
    "# specify date range as start date & number of days.\n",
    "start_date = datetime.date(2012,1,1)\n",
    "# NOTE: ndays must be >1 \n",
    "ndays = 2\n",
    "\n",
    "\n",
    "# directory where data files are stored \n",
    "if not RUN_ON_CLOUD:\n",
    "    datadir = '/data1/adac/mitgcm/netcdf/' + RegionName + '/'  # location of input netcdf files\n",
    "    outputdir = '/data1/adac/osse_output/' + RegionName + '/'  # location of OSSE outputs\n",
    "    figdir = '/data2/Dropbox/projects/adac/figures/' + RegionName + '/' # location of figures\n",
    "\n",
    "else:\n",
    "    datadir = '/home/jovyan/data/llc4320/' + RegionName + '/'  # location of input netcdf files\n",
    "    outputdir = '/home/jovyan/osse_output/' + RegionName + '/'  # location of OSSE outputs\n",
    "    figdir = '/home/jovyan/osse_output/figures/' + RegionName + '/' # location of figures\n",
    "    \n",
    "    \n",
    "# optional details for sampling (if not specified, reasonable defaults will be used)\n",
    "# NOTE!! mooring and sim_mooring are different:\n",
    "#    sim_mooring treats the mooring datapoints like a glider, \n",
    "#    whereas mooring interpolates directly to the mooring grid and should be faster\n",
    "sampling_details = {\n",
    "#     'SAMPLING_STRATEGY' : 'sim_glider', \n",
    "   'SAMPLING_STRATEGY' : 'trajectory_file', # options: sim_glider, sim_uctd or trajectory_file.add:  ASV\n",
    "#     'SAMPLING_STRATEGY' : 'mooring', # options: sim_glider, sim_uctd, sim_mooring or trajectory_file.add: ASV. \n",
    "#     'PATTERN' : 'lawnmower', # back-forth or lawnmower \n",
    "    'zrange' : [-1, -1000],  # depth range of T/S profiles (down is negative). * add U/V range? *\n",
    "#     'zmooring_TS' : list(range(-10,-1000,-10)) # instrument depths for moorings. T/S and U/V are the same.\n",
    "    'hspeed' : 0.25,  # platform horizontal speed in m/s (for glider, uCTD)\n",
    "    'vspeed' : 0.1, # platform vertical (profile) speed in m/s  (for glider, uCTD)\n",
    "   'trajectory_file' : '../data/survey_trajectory_ACC_SMST_glider.nc', # if SAMPLING_STRATEGY = 'trajectory_file', specify trajectory file\n",
    "    'AT_END' : 'reverse', # behaviour at and of trajectory: 'reverse', 'repeat' or 'terminate'. (could also 'restart'?)\n",
    "    'DERIVED_VARIABLES' : True # specify whether or not to process the derived variables (steric height, rotated velocity, vorticity) - slower and takes significant to derive/save the stored variables\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586d15a-b385-407e-8bfe-2288fd6523a3",
   "metadata": {},
   "source": [
    "#### Download & load model data and derived fields\n",
    "\n",
    "Based on [LLC4320](https://data.nas.nasa.gov/viz/vizdata/llc4320/index.html), the 1/48-degree global MITgcm simulation produced by the ECCO project. Ten regional cut-outs of the simulation are available on the [PO.DAAC](https://podaac.jpl.nasa.gov/datasetlist?ids=Processing+Levels&values=4+-+Gridded+Model+Output&search=Pre-SWOT+llc4320&view=list&provider=); the 4x4 degree regional domains are small enough to enable fairly easy downloads and processing. The data from the model were retrieved using download_llc4320.ipynb and saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c807660-4015-4593-a01f-0887ac81e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following two lines if troubleshooting osse_tools\n",
    "# del sys.modules['osse_tools']  \n",
    "# from osse_tools import download_llc4320_data, compute_derived_fields, get_survey_track, survey_interp\n",
    "\n",
    "    \n",
    "if not RUN_ON_CLOUD:\n",
    "\n",
    "    # download files:\n",
    "    download_llc4320_data(RegionName, datadir, start_date, ndays)\n",
    "\n",
    "    # derive & save new files with steric height & vorticity\n",
    "    if sampling_details['DERIVED_VARIABLES']:\n",
    "        compute_derived_fields(RegionName, datadir, start_date, ndays)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2a8ef6-85e5-4dad-b3ad-4f1b2945af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this tutorial: https://github.com/podaac/tutorials/blob/master/notebooks/Pre-SWOT_Numerical_Simulation_Demo.ipynb\n",
    "# => TODO: move all this to the .py\n",
    "    \n",
    "if RUN_ON_CLOUD:\n",
    "\n",
    "    from netrc import netrc\n",
    "    from urllib import request\n",
    "    from platform import system\n",
    "    from getpass import getpass\n",
    "    from http.cookiejar import CookieJar\n",
    "    from os.path import expanduser, join\n",
    "\n",
    "    # Authenticate with your Earthdata Login/URS credentials by configuring a .netrc file in your home directory.\n",
    "    \n",
    "    def setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n",
    "        netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "        try:\n",
    "            username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n",
    "        except (FileNotFoundError, TypeError):\n",
    "            print('Please provide your Earthdata Login credentials for access.')\n",
    "            print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n",
    "            username = input('Username: ')\n",
    "            password = getpass('Password: ')\n",
    "        manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "        manager.add_password(None, endpoint, username, password)\n",
    "        auth = request.HTTPBasicAuthHandler(manager)\n",
    "        jar = CookieJar()\n",
    "        processor = request.HTTPCookieProcessor(jar)\n",
    "        opener = request.build_opener(auth, processor)\n",
    "        request.install_opener(opener)\n",
    "\n",
    "    setup_earthdata_login_auth()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9e9c2c3-66c8-4933-b966-21f43626557d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this works\n",
    "fs.glob(f\"podaac-ops-cumulus-protected/{ShortName}/{target_file}\")\n",
    "# this throws an error\n",
    "# fs.cp(f\"podaac-ops-cumulus-protected/{ShortName}/{target_file}\", '/home/jovyan/data/')\n",
    "\n",
    "# this works - but is it effecient? could I just open remotely instead of storing \"locally\"? Yes, assume it's faster to d/l\n",
    "# s3.download(f, \"DEMO_FILES/\" + os.path.basename(f))\n",
    "fs.download(f\"podaac-ops-cumulus-protected/{ShortName}/{target_file}\", datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ee2e6ca-04f6-4610-ba63-26746530a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 ms, sys: 325 ms, total: 356 ms\n",
      "Wall time: 535 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d1 = xr.open_dataset(f\"{datadir}/{target_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4879a358-abbf-4ab7-b0f6-c485e284600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 508 ms, total: 3.04 s\n",
      "Wall time: 7.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d2 = xr.open_dataset(fs.open(f\"podaac-ops-cumulus-protected/{ShortName}/{target_file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ec299de7-c99a-464b-9b48-284d335bd998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLC4320_pre-SWOT_ACC_SMST_20120101.nc\n",
      "LLC4320_pre-SWOT_ACC_SMST_20120102.nc\n",
      "copying LLC4320_pre-SWOT_ACC_SMST_20120102.nc to local storage\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's3fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mosse_tools_cloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_llc4320_data\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# download files:\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdownload_llc4320_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRegionName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatadir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndays\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/oceanliner/testing/osse_tools_cloud.py:140\u001b[0m, in \u001b[0;36mdownload_llc4320_data\u001b[0;34m(RegionName, datadir, start_date, ndays)\u001b[0m\n\u001b[1;32m    133\u001b[0m         # use s3fs download to copy the file to local datadir \n\u001b[1;32m    134\u001b[0m         fs.download(f\"{s3path}/{ShortName}/{target_file}\", datadir)\n\u001b[1;32m    136\u001b[0m         # except:\n\u001b[1;32m    137\u001b[0m         #     try:\n\u001b[1;32m    138\u001b[0m         #         # local machine dir\n\u001b[1;32m    139\u001b[0m         #         filename_dir = os.path.join(datadir, target_file)\n\u001b[0;32m--> 140\u001b[0m         #         # TODO: test locally! THIS MIGHT BE WRONG!!!! \n\u001b[1;32m    141\u001b[0m         #         request.urlretrieve(f\"{s3path}/{ShortName}/{target_file}\", filename_dir)\n\u001b[1;32m    142\u001b[0m         #     except:\n\u001b[1;32m    143\u001b[0m         #         print(' ---- error - skipping this file')\n\u001b[1;32m    146\u001b[0m def begin_s3_direct_access():\n\u001b[1;32m    147\u001b[0m     \"\"\"Returns s3fs object for accessing datasets stored in S3.\"\"\"\n",
      "File \u001b[0;32m~/oceanliner/testing/osse_tools_cloud.py:125\u001b[0m, in \u001b[0;36mdownload_llc4320_data.<locals>.begin_s3_direct_access\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# TODO: MAKE SURE THIS DOESN'T HAPPEN EVERY TIME WE LOOP THROUGH:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.podaac.earthdata.nasa.gov/s3credentials\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms3fs\u001b[49m\u001b[38;5;241m.\u001b[39mS3FileSystem(key\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccessKeyId\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    126\u001b[0m                          secret\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecretAccessKey\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    127\u001b[0m                          token\u001b[38;5;241m=\u001b[39mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msessionToken\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    128\u001b[0m                          client_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion_name\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mus-west-2\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 's3fs' is not defined"
     ]
    }
   ],
   "source": [
    "del sys.modules['osse_tools_cloud']  \n",
    "from osse_tools_cloud import download_llc4320_data\n",
    "\n",
    "# download files:\n",
    "download_llc4320_data(RegionName, datadir, start_date, ndays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "897cdbc5-9dcc-4119-a517-8650b94ee93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3path = \"podaac-ops-cumulus-protected\"\n",
    "fs.download(f\"{s3path}/{ShortName}/{target_file}\", datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51104b4d-1865-4852-871e-83bab24ebdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frozen({'j_g': 395, 'i': 480, 'i_g': 480, 'j': 395, 'k': 88, 'k_u': 88, 'k_l': 88, 'k_p1': 89, 'nb': 2, 'time': 24})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95c8bf7e-9a43-47f5-bf88-c942f120f6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d48131e5-076b-4350-983f-b298eefa09d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShortName' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mShortName\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ShortName' is not defined"
     ]
    }
   ],
   "source": [
    "ShortName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7518a-1c05-484b-814e-0df0a60bb1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files to load:\n",
    "\n",
    "\n",
    "# # \n",
    "# if RUN_ON_CLOUD:\n",
    "#     # produce \"short name\" as listed on PO.DAAC\n",
    "#     shortname = 'MITgcm_LLC4320_Pre-SWOT_JPL_L4_' + RegionName + '_v1.0/'\n",
    "     \n",
    "#     # list of target files:\n",
    "#     remote_files = []\n",
    "#     for n in range(ndays):\n",
    "#         s3path = f's3://podaac-ops-cumulus-protected/{shortname}LLC4320_pre-SWOT_{RegionName}_{date_list[n].strftime(\"%Y%m%d\")}.nc'\n",
    "#         remote_files.append(s3.glob(s3path)[0])\n",
    "#     print(remote_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c39d3-812f-4cd7-a134-2545556ad9b8",
   "metadata": {},
   "source": [
    "### Cloud access using \"earthdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f2c2a7-cfd9-49ef-9290-9331f485eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.date(2012,1,1)\n",
    "ndays = 31\n",
    "ndays = 60\n",
    "ndays = 5\n",
    "# print((start_date, datetime.timedelta(ndays)))\n",
    "# print(start_date, start_date+datetime.timedelta(ndays))\n",
    "# print(f'\"{start_date}\", \"{start_date+datetime.timedelta(ndays)}\"')\n",
    "print(f'(\"{start_date}\", \"{start_date+datetime.timedelta(ndays)}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792022dc-c568-4fa1-bdba-e2cca87acd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the classes from earthdata\n",
    "from earthdata import Auth, DataCollections, DataGranules, Store\n",
    "\n",
    "auth = Auth()\n",
    "\n",
    "# First we try to use a .netrc, if it's not present we use the interactive login\n",
    "if not auth.login(strategy=\"netrc\"):\n",
    "    auth.login(strategy=\"interactive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eadf71-6fc2-4e4d-9f77-0285917b541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the DataGranules function from earthdata:\n",
    "\n",
    "\n",
    "\n",
    "GranuleQuery = DataGranules().parameters(\n",
    "    short_name = \"MITgcm_LLC4320_Pre-SWOT_JPL_L4_ACC_SMST_v1.0\",\n",
    "    temporal = (\"2012-01-01\", \"2012-01-06\")\n",
    "    # temporal = (\"2012-01-01\", \"2012-01-05\")\n",
    "    # temporal = f'(\"{start_date}\", \"{start_date+datetime.timedelta(ndays)}\")'\n",
    ")\n",
    "\n",
    "granules = GranuleQuery.get(1)\n",
    "\n",
    "for granule in granules:\n",
    "    # print(granule)\n",
    "    # pprint(granule)\n",
    "    display(granule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba7c3e-ab99-4341-9c88-babe09e9dd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e95d6-7975-4db9-a76c-ff913f3cc08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d0caa-107e-40b5-8ecb-ca7e1e24c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"target_files\" points to remote files\n",
    "target_files = [s3.open(file) for file in remote_files]\n",
    "target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae58408-a1a6-4b08-bcc3-86620ce67ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(target_files, parallel=True, drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe89d43-bf4b-4e01-a8f3-3d5b84e0ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50818d33-01c3-4194-8c5a-d61cd64b8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model data files. No not load U, V, or oceTAUX, oceTAUY as we will replace these with transformed versions \n",
    "date_list = [start_date + datetime.timedelta(days=x) for x in range(ndays)]\n",
    "target_files = [f'{datadir}LLC4320_pre-SWOT_{RegionName}_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "# ds = xr.open_mfdataset(target_files, parallel=True)\n",
    "ds = xr.open_mfdataset(target_files, parallel=True, drop_variables={'U', 'V', 'oceTAUX', 'oceTAUY'})\n",
    "\n",
    "# # rename rotated vectors data to original names\n",
    "#             # !! caution, skipping this step will cause great confusion later !!\n",
    "#             ds = ds.rename_vars({'U':'Utemp', 'V':'Vtemp'}).rename_vars({'U_r':'U', 'V_r':'V'})\n",
    "#             ds = ds.rename_vars({'oceTAUX':'TAUXtemp', 'oceTAUY':'TAUYtemp'}).rename_vars({'oceTAUX_r':'oceTAUX', 'oceTAUY_r':'oceTAUY'})\n",
    "\n",
    "#             # 5) get rid of all the original U/V/TAUX/TAUY variables (which have been renamed to *temp)\n",
    "#             ds = ds.drop_vars({'Utemp','Vtemp','TAUXtemp','TAUYtemp'})\n",
    "#             ds\n",
    "\n",
    "\n",
    "\n",
    "# load the corresponding derived fields\n",
    "derivedir = datadir + 'derived/'\n",
    "derived_files = [f'{derivedir}LLC4320_pre-SWOT_{RegionName}_derived-fields_{date_list[n].strftime(\"%Y%m%d\")}.nc' for n in range(ndays)] # list target files\n",
    "dsd = xr.open_mfdataset(derived_files)\n",
    "\n",
    "# merge the derived and raw data\n",
    "ds = ds.merge(dsd)\n",
    "\n",
    "# rename the transformed vector variables to their original names\n",
    "ds = ds.rename_vars({'U_transformed':'U', 'V_transformed':'V', \n",
    "                     'oceTAUX_transformed':'oceTAUX', 'oceTAUY_transformed':'oceTAUY'})\n",
    "# Convert lon, lat and z to index i, j and k with f_x, f_y and f_z\n",
    "# XC, YC and Z are the same at all times, so select a single time\n",
    "X = ds.XC.isel(time=0) \n",
    "Y = ds.YC.isel(time=0)\n",
    "# X = ds.XC\n",
    "# Y = ds.YC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed6703-b406-4d2c-b04c-2b73c82bc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a bunch of other vars we don't actually use - can comment this out if these are wanted\n",
    "ds = ds.drop_vars({'DXV','DYU', 'DXC','DXG', 'DYC','DYG', 'XC_bnds', 'YC_bnds', 'Zp1', 'Zu','Zl','Z_bnds', 'nb'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9597c2-b723-4a8f-a9b9-bb8002ee571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_details['zmooring_TS'] = list(range(-10,-1000,-100))\n",
    "sampling_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d3397-c4bf-430d-bf07-4dae3a064cc5",
   "metadata": {},
   "source": [
    "### Create & plot sampling track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f0f6d-6f28-46a1-84fb-edc3aeda271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sys.modules['osse_tools'] \n",
    "from osse_tools import download_llc4320_data, compute_derived_fields, get_survey_track, survey_interp\n",
    "\n",
    "# sampling_details['AT_END'] = 'repeat'\n",
    "# # sampling_details['AT_END'] = 'terminate'\n",
    "# sampling_details['AT_END'] = 'reverse'\n",
    "\n",
    "sampling_details['zmooring_TS'] = list(range(-10,-1000,-100))\n",
    "survey_track, survey_indices, sampling_parameters = get_survey_track(ds, sampling_details)\n",
    "\n",
    "# print specified sampling_details + any default values\n",
    "sampling_parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce76d0-e954-4d3f-a3ea-34a752b57686",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585a353-c3a7-465d-ae97-b1bdf859a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize track over a single model snapshot:\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "ssto = plt.pcolormesh(X,Y,ds.Theta.isel(k=0, time=0).values, shading='auto')\n",
    "if not (sampling_parameters['SAMPLING_STRATEGY'] == 'mooring' or sampling_parameters['SAMPLING_STRATEGY'] == 'sim_mooring'):\n",
    "    tracko = plt.scatter(survey_track.lon, survey_track.lat, c=survey_track.time-survey_track.time[0], cmap='Reds', s=0.75)\n",
    "    plt.colorbar(ssto).set_label('SST, $^o$C')\n",
    "    plt.colorbar(tracko).set_label('days from start')\n",
    "    plt.title('SST and survey track: ' + RegionName)\n",
    "else:\n",
    "    plt.plot(survey_track.lon, survey_track.lat, marker='*', c='r')\n",
    "    plt.title('SST and mooring location: ' + RegionName)\n",
    "\n",
    "    \n",
    "ax = plt.subplot(1,2,2)\n",
    "if not (sampling_parameters['SAMPLING_STRATEGY'] == 'mooring' or sampling_parameters['SAMPLING_STRATEGY'] == 'sim_mooring'):\n",
    "    plt.plot(survey_track.time, survey_track.dep, marker='.')\n",
    "else:\n",
    "    # not quite right but good enough for now.\n",
    "    # (times shouldn't increase with depth)\n",
    "    plt.scatter((np.tile(survey_track['time'], int(survey_track['dep'].data.size))),\n",
    "         np.tile(survey_track['dep'], int(survey_track['time'].data.size)),marker='.')             \n",
    "# plt.xlim([start_date + datetime.timedelta(days=0), start_date + datetime.timedelta(days=2)])\n",
    "    \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e11cc-a32d-4020-8d60-2c46f93c763e",
   "metadata": {},
   "source": [
    "### Interpolate data with the specified sampling pattern (this is where the magic happens!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93276e-31e5-472b-85b0-f790f11ed274",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sys.modules['osse_tools'] \n",
    "from osse_tools import survey_interp, get_survey_track\n",
    "start_time = time.time()\n",
    "subsampled_data, sgridded = survey_interp(ds, survey_track, survey_indices)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# NOTE! this breaks if we've already run \"compute\" on U, V.\n",
    "# can try a flag/test here if that's faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b399b-07b5-47f3-afd8-27571acd9c8a",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "Basic plots to show the interpolated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7267d1f-aa2d-4fdc-b1d7-16f26545d167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3d fields\n",
    "vbls3d = ['Theta','Salt','U','V','vorticity']\n",
    "ylim = [min(sgridded['depth'].values), max(sgridded['depth'].values)]\n",
    "# ylim = [-200, -1]\n",
    "\n",
    "nr = len(vbls3d) # # of rows\n",
    "fig,ax=plt.subplots(nr,figsize=(8,10),constrained_layout=True)\n",
    "\n",
    "\n",
    "for j in range(nr):\n",
    "    sgridded[vbls3d[j]].plot(ax=ax[j], ylim=ylim)\n",
    "    ax[j].plot(sgridded.time.data, -sgridded.KPPhbl.data, c='k')\n",
    "    ax[j].set_title(vbls3d[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45170e53-31c4-4e48-904e-5f996d8185d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## selected 2d fields\n",
    "j=0\n",
    "nr = 6 # # of rows\n",
    "fig,ax=plt.subplots(nr,figsize=(10,8),constrained_layout=True)\n",
    "\n",
    "\n",
    "# wind vectors\n",
    "ax[j].quiver(sgridded.time.data,0,sgridded.oceTAUX.data, sgridded.oceTAUY.data)\n",
    "ax[j].set_title('Wind stress')    \n",
    "ax[j].set_ylabel('N m-2')\n",
    "# SH \n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.steric_height-sgridded.steric_height.mean(), \n",
    "             sgridded.time.data,sgridded.steric_height_true-sgridded.steric_height_true.mean())\n",
    "ax[j].set_title('Steric height')\n",
    "ax[j].legend(['subsampled','true'])\n",
    "ax[j].set_ylabel('m')\n",
    "\n",
    "# SSH\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.Eta)\n",
    "ax[j].set_title('SSH')\n",
    "ax[j].set_ylabel('m')\n",
    "\n",
    "# MLD\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.KPPhbl)\n",
    "ax[j].set_title('MLD')\n",
    "ax[j].set_ylabel('m')\n",
    "ax[j].invert_yaxis()\n",
    "\n",
    "# surface heat flux\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.oceQnet, sgridded.time,sgridded.oceQsw)\n",
    "ax[j].set_title('Surface heat flux into the ocean')\n",
    "ax[j].legend(['total','shortwave'])\n",
    "ax[j].set_ylabel('W m-2')\n",
    "\n",
    "# surface FW flux\n",
    "j+=1\n",
    "ax[j].plot(sgridded.time,sgridded.oceFWflx)\n",
    "ax[j].set_title('Surface freshwater flux into the ocean') \n",
    "ax[j].set_ylabel('kg m-2 s-1')\n",
    "\n",
    "# horiz line:\n",
    "for j in range(nr):\n",
    "    ax[j].axhline(0, color='grey', linewidth=0.8)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c818a-4f43-4a3d-ae9e-980eebaa00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['PhiBot'].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52eaca3-5158-400c-ac67-f6f8121092ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgridded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3cf95-69c9-49ce-8cc9-e40d74b38a38",
   "metadata": {},
   "source": [
    "### Save interpolated data\n",
    "\n",
    "For both raw and gridded subsampled data, add attributes and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a72a8-4d21-49be-a4e9-cbeb478770e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add metadata to attributes\n",
    "attrs = sampling_parameters\n",
    "attrs['start_date'] = start_date.strftime('%Y-%m-%d')\n",
    "end_date = sgridded['time'].data[-1]\n",
    "attrs['end_date'] = np.datetime_as_string(end_date,unit='D')\n",
    "attrs['ndays'] = ndays\n",
    "\n",
    "# output filename base:\n",
    "# --- auto generated but can be set by user ---\n",
    "filename_out_base = (f'{outputdir}OSSE_{RegionName}_{sampling_parameters[\"SAMPLING_STRATEGY\"]}_{attrs[\"start_date\"]}_to_{attrs[\"end_date\"]}')\n",
    "print(filename_out_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7558b-4ef6-43d3-9037-6a9f54d2684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ subsampled data\n",
    "if sampling_parameters['SAMPLING_STRATEGY'] != 'mooring':\n",
    "    filename_out = filename_out_base + '_subsampled.nc'\n",
    "    print(f'saving to {filename_out}')\n",
    "    subsampled_data.attrs = attrs\n",
    "    netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "    dv_encoding={'zlib':True,  # turns compression on\\\n",
    "                'complevel':9,     # 1 = fastest, lowest compression; 9=slowest, highest compression \\\n",
    "                'shuffle':True,    # shuffle filter can significantly improve compression ratios, and is on by default \\\n",
    "                'dtype':'float32',\\\n",
    "                '_FillValue':netcdf_fill_value}\n",
    "    # save to a new file\n",
    "    # subsampled_data.to_netcdf(filename_out,format='netcdf4',encoding=dv_encoding)\n",
    "    subsampled_data.to_netcdf(filename_out,format='netcdf4')\n",
    "    !ls -ltrh {filename_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549dc91-7959-40cc-894f-5a957e92ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ gridded:\n",
    "filename_out = filename_out_base + '_gridded.nc'\n",
    "print(f'saving to {filename_out}')\n",
    "sgridded.attrs = attrs\n",
    "netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "dv_encoding={'zlib':True,  # turns compression on\\\n",
    "            'complevel':9,     # 1 = fastest, lowest compression; 9=slowest, highest compression \\\n",
    "            'shuffle':True,    # shuffle filter can significantly improve compression ratios, and is on by default \\\n",
    "            'dtype':'float32',\\\n",
    "            '_FillValue':netcdf_fill_value}\n",
    "# save to a new file\n",
    "# subsampled_data.to_netcdf(filename_out,format='netcdf4',encoding=dv_encoding)\n",
    "sgridded.to_netcdf(filename_out,format='netcdf4')\n",
    "!ls -ltrh {filename_out}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f399a299-68a6-40c0-823e-a195c7f52fc3",
   "metadata": {},
   "source": [
    "### Visualize interpolated data in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7f711-a7a9-4410-abed-bf017d60df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = plt.axes(projection='3d')\n",
    "fig.subplots_adjust(left=0.25, bottom=0.25)\n",
    "\n",
    "ax.set_xlabel('longitude', fontsize=15, rotation=150)\n",
    "ax.set_ylabel('latitude',fontsize=15)\n",
    "ax.set_zlabel('depth', fontsize=15, rotation=60)\n",
    "\n",
    "p = ax.scatter3D(subsampled_data.lon.data, subsampled_data.lat.data, subsampled_data.dep.data, c=subsampled_data.Theta.data, s=1)\n",
    "fig.colorbar(p).set_label('Temperature ($^o$C)')\n",
    "ax.set_title('Temperature interpolated to the survey track')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b52e7-2783-410e-a7d9-d396c9ed9699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129aaedd-15a9-4bd9-955b-bedc1d231070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbe965-9b52-4b07-a22b-a1af4ef6bbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb137f50-fd37-4b46-b0f8-883c9d044f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2e205-17a2-4b5e-a191-5f5942b8ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9b8f3-18fe-46c2-8bc2-9ec55abae09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241d3d4-6794-4e83-83da-b80d3dbf4915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a92949-77c2-4533-90b0-8c5acec3b9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4baaf300-cb93-4803-9635-784664d68543",
   "metadata": {},
   "source": [
    "#### EXPLORE DIFFERENT INTERPOLATION METHODS \n",
    "\n",
    "It's re-gridding the 3-d interpolated data that is slow. (2d is trivial)\n",
    "\n",
    "Which part?\n",
    "the line this_var = subsampled_data[vbl].data.compute().copy() \n",
    "is SUPER slow (20 sec) for V, slow (9 sec) for U, moderate (2 sec) for theta and salt, 0 for SH and vorticity\n",
    "\n",
    "I guess because making those regridding computations is slow?\n",
    "- is the regridding necessary? (yes, to do it right - but could check)\n",
    "- can we regrid when we derive fields? (yes - but not sure it's any faster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845653b-282a-42cb-a7cb-91200514304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL CODE in osse_tools.py\n",
    "start_time = time.time()\n",
    "# vbls3d = ['U','V', 'Theta','Salt','vorticity','steric_height']\n",
    "# vbls2d = ['steric_height_true', 'Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "\n",
    "\n",
    "vbls3d = ['U_c','V_c', 'U', 'V']\n",
    "vbls3d = ['U', 'V']\n",
    "\n",
    "# -----\n",
    "\n",
    "# ------Regrid the data to depth/time (3-d fields) or subsample to time (2-d fields)\n",
    "print('Gridding the interpolated data...')\n",
    "# get times associated with profiles:\n",
    "SAMPLING_STRATEGY = survey_track['SAMPLING_STRATEGY']\n",
    "if SAMPLING_STRATEGY == 'sim_mooring':\n",
    "    # - for mooring, use the subsampled time grid:\n",
    "    times = np.unique(subsampled_data.time.values)\n",
    "else:\n",
    "    # -- for glider/uctd, take the shallowest & deepest profiles (every second value, since top/bottom get sampled twice for each profile)\n",
    "    time_deepest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True).values[0:-1:2]\n",
    "    time_shallowest = subsampled_data.time.where(subsampled_data.dep == subsampled_data.dep.max(), drop=True).values[0:-1:2]\n",
    "    times = np.sort(np.concatenate((time_shallowest, time_deepest)))\n",
    "    # this results in a time grid that may not be uniformly spaced, but is correct\n",
    "    # - for a uniform grid, use the mean time spacing - may not be perfectly accurate, but is evenly spaced\n",
    "    dt = np.mean(np.diff(time_shallowest))/2 # average spacing of profiles (half of one up/down, so divide by two)\n",
    "    times_uniform = np.arange(survey_track.n_profiles.values*2) * dt\n",
    "    print(\"--- deal with times: %s seconds ---\" % int(time.time() - start_time))\n",
    "# nt is the number of profiles (times):\n",
    "nt = len(times)  \n",
    "# xgr is the vertical grid; nz is the number of depths for each profile\n",
    "# depths are negative, so sort in reverse order using flip\n",
    "zgridded = np.flip(np.unique(subsampled_data.dep.data))\n",
    "nz = int(len(zgridded))\n",
    "\n",
    "# -- initialize the dataset:\n",
    "sgridded = xr.Dataset(\n",
    "    coords = dict(depth=([\"depth\"],zgridded),\n",
    "              time=([\"time\"],times))\n",
    ")\n",
    "# -- 3-d fields: loop & reshape 3-d data from profiles to a 2-d (depth-time) grid:\n",
    "# first, extract each variable, then reshape to a grid\n",
    "# add U and V to the list:\n",
    "# vbls3d.append('U')\n",
    "# vbls3d.append('V')\n",
    "\n",
    "\n",
    "for vbl in vbls3d:\n",
    "    print(vbl)\n",
    "    start_time = time.time()\n",
    "    this_var = subsampled_data[vbl].data.compute().copy() \n",
    "    print(\"--- this_var = subsampled_data[vbl].data.compute().copy(): %s seconds ---\" % int(time.time() - start_time))\n",
    "\n",
    "    # reshape to nz,nt\n",
    "    start_time = time.time()\n",
    "    this_var_reshape = np.reshape(this_var,(nz,nt), order='F') # fortran order is important!\n",
    "    print(\"--- this_var_reshape = np.reshape(this_var,(nz,nt), order='F'): %s seconds ---\" % int(time.time() - start_time))\n",
    "    # for platforms with up & down profiles (uCTD and glider),\n",
    "    # every second column is upside-down (upcast data)\n",
    "    # starting with the first column, flip the data upside down so that upcasts go from top to bottom\n",
    "    start_time = time.time()\n",
    "    if SAMPLING_STRATEGY != 'sim_mooring':\n",
    "        this_var_fix = this_var_reshape.copy()\n",
    "        #this_var_fix[:,0::2] = this_var_fix[-1::-1,0::2] \n",
    "        this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]  # Starting with SECOND column\n",
    "        sgridded[vbl] = ((\"depth\",\"time\"), this_var_fix)\n",
    "    elif SAMPLING_STRATEGY == 'sim_mooring':\n",
    "        sgridded[vbl] = ((\"depth\",\"time\"), this_var_reshape)\n",
    "        \n",
    "    print(\"--- this_var_fix[:,1::2] = this_var_fix[-1::-1,1::2]: %s seconds ---\" % int(time.time() - start_time))\n",
    "print(\"--- 3d vars: TOTAL %s seconds ---\" % int(time.time() - start_time))\n",
    "\n",
    "# # for sampled steric height, we want the value integrated from the deepest sampling depth:\n",
    "# sgridded['steric_height'] = ((\"time\"), sgridded['steric_height'].isel(depth=nz-1))\n",
    "# # rename to \"sampled\" for clarity\n",
    "# sgridded.rename_vars({'steric_height':'steric_height_sampled'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9783324-6a8d-434c-9c4e-97d13e90948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f37da3-ae79-43e7-b5b8-e499c8e133bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataset to contain the interpolated data, and interpolate\n",
    "subsampled_data = xr.Dataset(\n",
    "    dict(\n",
    "        t = xr.DataArray(survey_track.time, dims='points'), # call this time, for now, so that the interpolation works\n",
    "        lon = xr.DataArray(survey_track.lon, dims='points'),\n",
    "        lat = xr.DataArray(survey_track.lat, dims='points'),\n",
    "        dep = xr.DataArray(survey_track.dep, dims='points'),\n",
    "        points = xr.DataArray(survey_track.points, dims='points')\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Interpolating model fields to the sampling track...')\n",
    "# loop & interpolate through 3d variables:\n",
    "vbls3d = ['Theta','Salt','vorticity','steric_height']\n",
    "for vbl in vbls3d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "# Interpolate U and V from i_g, j_g to i, j, then interpolate:\n",
    "# Get u, v\n",
    "grid = Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "                        'Y':{'center': 'j', 'left': 'j_g'},\n",
    "                        'Z':{'center': 'k'}})\n",
    "U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "subsampled_data['U'] = U_c.interp(survey_indices)\n",
    "subsampled_data['V'] = V_c.interp(survey_indices)    \n",
    "# # subsampled_data['U_test']=ds['U'].interp(survey_indices)\n",
    "# # ds\n",
    "# survey_indices\n",
    "# grid = Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "#                         'Y':{'center': 'j', 'left': 'j_g'},\n",
    "#                         'Z':{'center': 'k'}})\n",
    "# U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "# V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "subsampled_data['U_test']=ds['U'].interp(survey_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20366cd3-07e2-4218-8e53-51133715dae8",
   "metadata": {},
   "source": [
    "### TEST! is it faster to interpolate mooring data to a grid rather than as \"points\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557e147-793a-42f0-8b82-90812334400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48423e17-662d-4402-8f33-2de9810008ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate to xi,yi,ti,zi\n",
    "xi = sampling_parameters['xmooring']\n",
    "yi = sampling_parameters['ymooring']\n",
    "ti = ds['time']\n",
    "zi = sampling_parameters['zmooring_TS']\n",
    "\n",
    "# ---- copied ----\n",
    "# Convert lon, lat and z to index i, j and k with f_x, f_y and f_z\n",
    "# XC, YC and Z are the same at all times, so select a single time\n",
    "X = ds.XC.isel(time=0) \n",
    "Y = ds.YC.isel(time=0)\n",
    "i = ds.i\n",
    "j = ds.j\n",
    "z = ds.Z.isel(time=0)\n",
    "k = ds.k\n",
    "f_x = interpolate.interp1d(X[0,:].values, i)\n",
    "f_y = interpolate.interp1d(Y[:,0].values, j)\n",
    "f_z = interpolate.interp1d(z, k, bounds_error=False)\n",
    "# ---- copied ----\n",
    "    \n",
    "# survey_indices_mooring= xr.Dataset(\n",
    "#         dict(\n",
    "#             i = xr.DataArray(f_x(xmooring), dims='points'),\n",
    "#             j = xr.DataArray(f_y(ymooring), dims='points'),\n",
    "#             k = xr.DataArray(f_z(survey_track.dep), dims='points'),\n",
    "#             time = xr.DataArray(survey_track.time, dims='points'),\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21307610-5143-430a-ae99-b681406f5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- initialize the dataset:\n",
    "# mgridded = xr.Dataset(\n",
    "#     coords = dict(depth=([\"depth\"],zgridded),\n",
    "#               time=([\"time\"],times))\n",
    "# # )\n",
    "# vbls3d = ['Theta','Salt','vorticity','steric_height', 'U', 'V']\n",
    "# vbls3d = ['Theta','Salt','vorticity','steric_height', 'U_c', 'V_c']\n",
    "# for vbl in vbls3d:\n",
    "#     print('interpolating ' + vbl)\n",
    "#     mgridded[vbl]=ds[vbl].interp(i=f_x(xi), j=f_y(yi), k=f_z(zi), time=ti).compute()\n",
    "   \n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c65cde-d617-46a3-96cc-ac389f86aa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee7c41-1b35-4840-9c28-e2725d50aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(np.transpose(dum.data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945abe3e-2702-4a81-8723-92483ed7116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgridded['Theta'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b31dd-0afe-4364-9a98-d08ce6f47f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try regridding U/V earlier (when deriving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698cfe38-3200-4622-a23a-905764ef10f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgridded.oceTAUX.plot()\n",
    "plt.quiver(sgridded.time.data,0,sgridded.oceTAUX.data, sgridded.oceTAUY.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59abf779-f569-49e5-a21f-2d89eae18a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806280d-f085-4789-a72f-e7a729e64f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae916f1-090b-487d-a586-f4b9d1a32823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to compare the two methods \n",
    "# ( not quite working )\n",
    "tpl = ds['time']/24,sss[:,0,0,0]\n",
    "ttt=ds['time']\n",
    "# ipl = ( subsampled_data.dep.values == subsampled_data.dep.values.min() )\n",
    "# plt.plot(subsampled_data.time(ipl),subsampled_data.Salt.sel(ipl).data,'-')\n",
    "# plt.plot(subsampled_data.time,subsampled_data.Salt.data,'-')\n",
    "plt.plot(tgr,np.transpose(dum),'-')\n",
    "plt.plot(ttt[:],sss[:,0,0,0],'k--')\n",
    "# plt.legend('survey_interp method','direct interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043f1a9-8daf-453f-9571-f62cf0cba80e",
   "metadata": {},
   "source": [
    "### Visualize steric height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9938dd8-ac0c-4f88-b978-9ee45f9a283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sh_anom = sh_true.values - sh_true.values.mean()\n",
    "plt.figure(figsize=(7,5))\n",
    "sho = plt.scatter(survey_track.lon, survey_track.lat, c=sh_anom)\n",
    "plt.title('Steric height anomaly')\n",
    "plt.colorbar(sho).set_label('m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205e49e-466a-453d-9c1a-8e1dd7516012",
   "metadata": {},
   "source": [
    "### Comparison of true vs sampled steric height\n",
    "Plot comparing the \"true\" steric height along the track and the subsampled steric height, which is computed by integrating the specific volume anomaly for each subsampled profile from its deepest sampling depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcc511-d507-4d6f-9542-6ed53eae79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# truth:\n",
    "plt.plot(sh_true.time, sh_true.values - sh_true.values.mean())\n",
    "# get index of the deepest sampling depths\n",
    "i = ( subsampled_data.dep.values == subsampled_data.dep.values.min() )\n",
    "plt.plot(subsampled_data.time.values[i], subsampled_data.steric_height.values[i] - subsampled_data.steric_height.values[i].mean(),'.-')\n",
    "plt.title('Steric height anomaly along the survey track')\n",
    "plt.legend(['truth','subsampled data'])\n",
    "plt.xlabel('time, days')\n",
    "plt.ylabel('steric height anom., m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b077123-0bd2-4a16-a014-2cf888ab17e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc3c72-388f-4297-8736-e893fa4f7020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83e3db-3235-4353-87e6-dcf53ca86c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a74e81-f329-42aa-b2b9-b8842f25a98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249b50a-28d6-448d-9b81-e2fe9f837815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48b496-0dfc-466d-97c8-655b2102591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE  BELOW\n",
    "\n",
    "# interpolation:\n",
    "subsampled_data = xr.Dataset()  \n",
    "\n",
    "# loop & interpolate through 3d variables:\n",
    "vbls3d = ['Theta','Salt','vorticity']\n",
    "# vbls3d = ['Theta']\n",
    "for vbl in vbls3d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "# # Interpolate U and V from i_g, j_g to i, j, then interpolate:\n",
    "# U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "# V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "# subsampled_data['U'] = U_c.interp(survey_indices)\n",
    "# subsampled_data['V'] = V_c.interp(survey_indices)\n",
    "\n",
    "subsampled_data['lon']=survey_track.lon\n",
    "subsampled_data['lat']=survey_track.lat\n",
    "subsampled_data['dep']=survey_track.dep\n",
    "subsampled_data['time']=survey_track.time \n",
    "\n",
    "# loop & interpolate through 2d variables:\n",
    "vbls2d = ['Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux', 'oceTAUX', 'oceTAUY']\n",
    "vbls2d = ['Eta', 'Depth']\n",
    "# create 2-d survey track by removing the depth dimension\n",
    "survey_indices_2d =  survey_indices.drop_vars('k')\n",
    "for vbl in vbls2d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices_2d)\n",
    "# survey_indices_2d.i.plot()\n",
    "# ds['KPPhbl'].interp(survey_indices_2d).plot()\n",
    "\n",
    "# interp\n",
    "# this returns a value at every timestep (points) - very high resolution\n",
    "# - could subsample to the model time grid \n",
    "\n",
    "\n",
    "# plot\n",
    "# plt.plot(sh_true.time, sh_true.values - sh_true.values.mean())\n",
    "# plt.plot(subsampled_data.time, ssh - ssh.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578cb0a-922b-4bfc-a979-d27c06090b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data = xr.Dataset() \n",
    "\n",
    "# loop & interpolate through 3d variables:\n",
    "vbls3d = ['Theta','Salt','vorticity','steric_height']\n",
    "for vbl in vbls3d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices)\n",
    "# Interpolate U and V from i_g, j_g to i, j, then interpolate:\n",
    "# Get u, v\n",
    "grid = Grid(ds, coords={'X':{'center': 'i', 'left': 'i_g'}, \n",
    "                        'Y':{'center': 'j', 'left': 'j_g'},\n",
    "                        'Z':{'center': 'k'}})\n",
    "U_c = grid.interp(ds.U, 'X', boundary='extend')\n",
    "V_c = grid.interp(ds.V, 'Y', boundary='extend')\n",
    "subsampled_data['U'] = U_c.interp(survey_indices)\n",
    "subsampled_data['V'] = V_c.interp(survey_indices)\n",
    "\n",
    "\n",
    "# add lat/lon/time to dataset\n",
    "subsampled_data['lon']=survey_track.lon\n",
    "subsampled_data['lat']=survey_track.lat\n",
    "subsampled_data['dep']=survey_track.dep\n",
    "subsampled_data['time']=survey_track.time  \n",
    "\n",
    "# loop & interpolate through 2d variables:\n",
    "vbls2d = ['Eta', 'KPPhbl', 'PhiBot', 'oceFWflx', 'oceQnet', 'oceQsw', 'oceSflux']\n",
    "# create 2-d survey track by removing the depth dimension\n",
    "survey_indices_2d =  survey_indices.drop_vars('k')\n",
    "for vbl in vbls2d:\n",
    "    subsampled_data[vbl]=ds[vbl].interp(survey_indices_2d)   \n",
    "# taux & tauy must be treated like U and V\n",
    "oceTAUX_c = grid.interp(ds.oceTAUX, 'X', boundary='extend')\n",
    "oceTAUY_c = grid.interp(ds.oceTAUY, 'Y', boundary='extend')\n",
    "subsampled_data['oceTAUX'] = oceTAUX_c.interp(survey_indices_2d)\n",
    "subsampled_data['oceTAUY'] = oceTAUY_c.interp(survey_indices_2d)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235f097-7730-4dcf-9448-c720a6f3ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61247c87-ea9e-4b8a-8101-02a8e310b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sh_true.time, sh_true.values - sh_true.values.mean())\n",
    "plt.plot(subsampled_data.time, ssh - ssh.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855af41-ccd8-4409-898b-bd3e76332862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mooring test\n",
    "\n",
    "model_xav = ds.XC.cccc.mean(dim='i').values\n",
    "model_yav = ds.YC.isel(time=0, i=0).mean(dim='j').values\n",
    "\n",
    "xmooring = model_xav # default lat/lon is the center of the domain\n",
    "ymooring = model_yav\n",
    "zmooring_TS = [-1, -10, -50, -100] # depth of T/S instruments\n",
    "zmooring_UV = [-1, -10, -50, -100, -200, -300, -400, -500] # depth of U/V instruments\n",
    "\n",
    "\n",
    "ts = ds.time\n",
    "n_samples = ts.size\n",
    "n_depths_TS = np.size(zmooring_TS)\n",
    "n_depths_UV = np.size(zmooring_UV)\n",
    "# depth sampling - different for TS and UV\n",
    "zs_TS = np.transpose(np.tile(zmooring_TS,(n_samples,1)))\n",
    "zs_UV = np.transpose(np.tile(zmooring_UV,(n_samples,1)))\n",
    "xs_TS = xmooring * np.ones([n_depths_TS, n_samples])\n",
    "ys_TS = ymooring * np.ones([n_depths_TS, n_samples])\n",
    "xs_UV = xmooring * np.ones([n_depths_TS, n_samples])\n",
    "ys_UV = ymooring * np.ones([n_depths_TS, n_samples])\n",
    "\n",
    "survey_track = xr.Dataset(\n",
    "    dict(\n",
    "        lon = xr.DataArray(xs_TS,dims='points'),\n",
    "        lat = xr.DataArray(ys_TS,dims='points'),\n",
    "        dep = xr.DataArray(zs_TS,dims='points'),\n",
    "        time = xr.DataArray(ts_TS,dims='points')\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8841e-fab5-4e62-b6c5-6d885667da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.broadcast_to([-1, -10, -50, -100],(2,4))\n",
    "np.broadcast_to([-1, -10, -50, -100],(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262f9d3-aeb6-4d36-8481-84b2e31b5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.broadcast_to(zmooring_TS,(2,4))\n",
    "# np.tile(zmooring_TS.transpose,(2,1))\n",
    "# np.tile(np.transpose(zmooring_TS),(1,n_samples)).shape\n",
    "# xs_TS.shape\n",
    "# [-1, -10, -50, -100]\n",
    "# zmooring_TS\n",
    "\n",
    "# np.transpose(np.array(zmooring_TS)).shape \n",
    "np.transpose(np.tile(zmooring_TS,(n_samples,1))).shape\n",
    "xs_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7d2d1-df54-45c5-86b8-06d4f253b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801311b8-64f3-469e-8a57-3b543588d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(subsampled_data.time,subsampled_data.dep,c=subsampled_data.Theta)\n",
    "plt.plot(subsampled_data.time,-subsampled_data.Depth,c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39b372-d0a4-443c-9b7d-bfee4d0911a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_true.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a7751-cb24-402d-ab9a-cfdf5d47458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we interpolated everything to the \"points\" - one datapoint per sample\n",
    "# but we may also want a cleaner (gridded) output product that has been reshaped into \n",
    "# profiles (i.e., a X x Z x T array)\n",
    "# (This might not make sense for glider data, since profiles are likely > one model gridpoint)\n",
    "\n",
    "# separate these into \"subsampled_points\" and \"subsampled_profiles\"?\n",
    "\n",
    "# a couple ways to determine the profiles:\n",
    "# - one per down / one per up, with time as either the start/mean/end\n",
    "# - one per down-up, with the time as the deepest point\n",
    "# - maybe others? would be useful to get feedback on this...\n",
    "\n",
    "# use \"where\" to determine the indices of the start/end (shallowest/deepest) of each profile:\n",
    "\n",
    "# subsampled_data.steric_height.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True)\n",
    "# subsampled_deepest = subsampled_data.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True)\n",
    "\n",
    "# plt.plot(subsampled_data.dep.values[ideep])\n",
    "\n",
    "# this is the DEEPEST point only. \n",
    "subsampled_data = subsampled_data\n",
    "dum = subsampled_data.where(subsampled_data.dep == subsampled_data.dep.min(), drop=True)\n",
    "shall = subsampled_data.where(subsampled_data.dep == subsampled_data.dep.max(), drop=True)\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(dum.lon,dum.lat,'.')\n",
    "plt.plot(shall.lon,shall.lat,'.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a08c54-6caa-44f1-8c3d-3b4ca1997398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# index?\n",
    "ishallow = ( subsampled_data.dep.values == subsampled_data.dep.values.max() )\n",
    "ideep = ( subsampled_data.dep.values == subsampled_data.dep.values.min() )\n",
    "# boolean to index\n",
    "ishallow = [ishallow for ishallow, x in enumerate(ishallow) if x]\n",
    "ideep = [ideep for ideep, x in enumerate(ideep) if x]\n",
    "\n",
    "t_profiles = subsampled_data['time'].isel(points=ishallow)\n",
    "t_profiles.plot()\n",
    "z = np.unique(subsampled_data['dep'])\n",
    "\n",
    "# # # initialize the dataset:\n",
    "# subsampled_profile = xr.Dataset(\n",
    "#     coords={\n",
    "#         \"time\": t_profiles,\n",
    "#         \"depth\": z\n",
    "#     },\n",
    "#     \"Salt\": ((\"time\", \"depth\"), []),\n",
    "# )\n",
    "\n",
    "\n",
    "# loop through each profile:\n",
    "\n",
    "pr = []\n",
    "for n in np.arange(np.size(ishallow)):\n",
    "    i = np.arange(ishallow[n],ideep[n])\n",
    "    # append\n",
    "    pr = np.append(pr,subsampled_data['Salt'].isel(points=i))\n",
    "    \n",
    "#     # loop & interpolate through 3d variables:\n",
    "#     for vbl in vbls3d:\n",
    "# #         subsampled_profile[vbl] = subsampled_data[vbl].isel(points=i)\n",
    "#         dum = subsampled_data[vbl].isel(points=i)\n",
    "\n",
    "pr   \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ca9ed-658f-43a8-8427-52a7c967d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f07c6-148d-441c-84dc-34a4abb3b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa038ce-4bb9-488a-a20a-ae3165d88fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # n=0\n",
    "# # i = np.arange(ishallow[n],ideep[n])\n",
    "# # start = time.time()\n",
    "# # dum = subsampled_data.Theta.isel(points=i)\n",
    "# # end = time.time()\n",
    "# # print(\"The time of execution of above program is :\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb24ae-e407-4df7-9226-67e33afb238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data.groupby(\"dep\").mean().scatter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0313b4-255f-4312-a0da-ddc545c23876",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = xr.Dataset(\n",
    "    {\n",
    "        \"Salt\": ((\"time\", \"depth\"), [subsampled_data.Salt.isel(points=i)]),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": [1],\n",
    "        \"depth\": z,\n",
    "    },\n",
    "    \n",
    ")\n",
    "ss\n",
    "# \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298ace1-2bcf-46f9-af55-3f934f089ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c76ee-cce1-4522-afcc-765104c8df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e26b84-7ae1-4942-aa78-31f8cc7bb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {**sampling_details}\n",
    "\n",
    "\n",
    "def test_code(sampling_details):\n",
    "    for key, value in sampling_details.items():\n",
    "#         print(key , '=' , value , '')\n",
    "#         print(type(key))\n",
    "        if isinstance(value,str):\n",
    "            exec(key + '=\"' + value + '\"',None, globals())\n",
    "#             print(key)\n",
    "            \n",
    "        if isinstance(value,list):\n",
    "            exec(key + '=' + str(value) + '',locals(), globals())\n",
    "#             print(key)\n",
    "            \n",
    "    print(zrange+1000)\n",
    "    return zrange\n",
    "    \n",
    "ddd = test_code(sampling_details)\n",
    "# test_code(sampling_details)\n",
    "ddd\n",
    "# for key in sampling_details:\n",
    "#     exec(key + '=' + '\"' + sampling_details[key] + '\"')\n",
    "\n",
    "# isinstance(sampling_details['zrange'],list)\n",
    "# isinstance(sampling_details['SAMPLING_STRATEGY'],list)\n",
    "# isinstance?\n",
    "\n",
    "# type(sampling_details['zrange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e96045-23f9-43ba-8ac0-ecc932513a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#             # 3) merge transformed data with ds\n",
    "#             print('merging with ds')\n",
    "#             print(ds)\n",
    "#             print(oceTAUX_r)\n",
    "#             ds = ds.merge(U_r.to_dataset(name='U_r')).merge(V_r.to_dataset(name='V_r'))\n",
    "#             ds = ds.merge(oceTAUX_r.to_dataset(name='oceTAUX_r')).merge(oceTAUY_r.to_dataset(name='oceTAUY_r'))\n",
    "\n",
    "#             # 4) rename transformed data to original names (renaming to 'temp' first)\n",
    "#             # !! caution, skipping this step will cause great confusion later !!\n",
    "#             ds = ds.rename_vars({'U':'Utemp', 'V':'Vtemp'}).rename_vars({'U_r':'U', 'V_r':'V'})\n",
    "#             ds = ds.rename_vars({'oceTAUX':'TAUXtemp', 'oceTAUY':'TAUYtemp'}).rename_vars({'oceTAUX_r':'oceTAUX', 'oceTAUY_r':'oceTAUY'})\n",
    "           \n",
    "\n",
    "#             # 6) get rid of all the original U/V/TAUX/TAUY variables (which have been renamed to *temp)\n",
    "#             ds = ds.drop_vars({'Utemp','Vtemp','TAUXtemp','TAUYtemp'})\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
